用户程序生成的每个地址都是虚拟地址（every address generated by a user program is a virtual address）。操作系统只是为每个进程提供一个假象，具体来说，就是它拥有自己的大量私有内存。在一些硬件帮助下，操作系统会将这些假的虚拟地址变成真实的物理地址，从而能够找到想要的信息。

## Ch13 抽象: 地址空间

从早起的OS开始, 看这一路的演变.

### 13.1 早期系统

- ![](assets/Pasted%20image%2020230328150336.png)

从内存来看，早期的机器并没有给用户提供什么抽象。基本上，机器的物理内存看起来如图 13.1 所示。
操作系统曾经是一组函数（实际上是一个库），在内存中（在本例中，从物理地址 0 开始），然后有一个正在运行的程序（进程），目前在物理内存中（在本例中，从物理地址 64KB 开始)，并使用剩余的内存。这里几乎没有抽象，用户对操作系统的要求也不多。
这个时候只能运行一个程序.

### 13.2 多道程序和时分共享

过了一段时间，由于机器昂贵，人们开始更有效地共享机器. 多道程序（multiprogramming）
系统时代开启. 其中多个进程在给定时间准备运行，比如当有一个进程在等待 I/O 操作的时候，
操作系统会切换这些进程，这样增加了 CPU 的有效利用率(utilization). 那时候，效率
(efficiency)的提高尤其重要.

但很快，人们开始对机器要求更多，分时系统的时代诞生了. 程序员厌倦了长时间的（因此也是低效率的）编程—调试循环, 交互性（interactivity）变得很重要，因为许多用户可能同时在使用机器，每个人都在等待（或希望）他们执行的任务及时响应.

一种实现时分共享的方法，是让一个进程单独占用全部内存运行一小段时间, 然后停止它，并将它所有的状态信息保存在磁盘上（包含所有的物理内存），加载其他进程的状态信息，再运行一段时间，这就实现了某种比较粗糙的机器共享.
这种方法有一个问题：太慢了，特别是当内存增长的时候。 在进程切换的时候，我们仍然将进程信息放在内存中(不在放到磁盘中)，这样操作系统可以更有效率地实现时分共享.

- ![](assets/Pasted%20image%2020230328151227.png)

随着时分共享变得更流行，人们对操作系统又有了新的要求。特别是多个程序同时驻留在内存中，使保护（protection）成为重要问题。人们不希望一个进程可以读取其他进程的内存，更别说修改了。

### 13.3 地址空间

操作系统需要提供一个易用（easy to use）的物理内存抽象。这个抽象叫作地址空间（address space），是运行的程序看到的系统中的内存.
理解这个基本的操作系统内存抽象，是了解内存虚拟化的关键。

一个进程的地址空间包含运行的程序的所有内存状态。比如：程序的`代码`（code，指令）必须在内存中，因此它们在地址空间里。当程序在运行的时候，利用`栈`（stack）来保存当前的函数调用信息，分配空间给局部变量，传递参数和函数返回值。最后，`堆`（heap）用于管理动态分配的、用户管理的内存，就像你从 C 语言中调用 malloc()或面向对象语言（如 C ++或 Java）中调用 new 获得内存。当然，还有其他的东西（例如，静态初始化的变量），但现在假设只有这 3 个部分：代码、栈和堆。

- ![](assets/Pasted%20image%2020230328151912.png)

代码区的大小不会变, 堆和栈在运行中, 是可以变大或变小的(内存的申请分配与释放).

当我们`描述地址空间时`，所描述的是操作系统提供给运行程序的`抽象`（abstract）。
`程序不在物理地址 0～16KB 的内存`中，而是加载在`任意的物理地址`。
回顾图 13.2 中的进程A、B 和 C，你可以看到每个进程如何加载到内存中的不同地址. ABC三个进程每个进程的内部结构, 就是图13.3的样子.

关键问题：如何虚拟化内存
操作系统如何在单一的物理内存上为多个运行的进程（所有进程共享内存）构建一个私有的、可能很大的地址空间的抽象？

当操作系统在做这个工作时，我们说操作系统在虚拟化内存（virtualizing memory), 因为每个运行着的程序都认为自己被加载到特定地址(例如 0)的内存中, 并且拥有整个内存地址空间(如32位或 64 位).  但实际上在物理内存中, 它只有一小块而已(结合图13.2和13.3).

例如，当图 13.2 中的进程 A 尝试在`地址 0`(称为`虚拟地址`, virtual address)执行加载操作时，然而操作系统在`硬件的支持`下，出于某种原因，必须确保不是加载到`物理地址 0`，而是物理地址 320KB（这是 A 载入实际内存的地址）。这是内存虚拟化的关键，这是世界上每一个现代计算机系统的基础。

>隔离是建立可靠系统的关键原则。如果两个实体相互隔离，这意味着一个实体的失败不会影响另一个实体。操作系统力求让进程彼此隔离，从而防止相互造成伤害。通过内存隔离，操作系统进一步确保运行程序不会影响底层操作系统的操作。
>微内核也是这种原则的产物. 一个内核模块崩溃, 系统整体不会崩溃.

### 13.4 目标

虚拟内存(VM)系统的一个主要目标是`透明(transparency)`. 
操作系统实现虚拟内存的方式，应该让运行的程序看不见。因此，程序不应该感知到内存被虚拟化的事实，相反，程序的行为就好像它拥有自己的私有物理内存。在幕后，操作系统（和硬件）完成了所有的工作，让不同的工作复用内存，从而实现这个假象。

虚拟内存的另一个目标是`效率(efficiency)`. 
操作系统应该追求虚拟化尽可能高效(efficient)，包括时间上（即不会使程序运行得更慢）和空间上（即不需要太多额外的内存来支持虚拟化）。在实现高效率虚拟化时，操作系统将不得不依靠硬件支持，包括 TLB 这样的硬件功能.


虚拟内存第三个目标是`保护(protection)`. 
操作系统应确保进程受到保护(protect), 不会受其他进程影响, 操作系统本身也不会受进程影响. 当一个进程执行加载、存储或指令提取时, 它不会以任何方式访问或影响任何其他进程或操作系统本身的内核内存(即在这个进程的地址空间之外的任何内容). 因此, 保护让我们能够在进程之间提供隔离(isolation)的特性, 每个进程都应该在自己的独立环境中运行, 避免其他出错或恶意进程的影响.

> 你看到的所有地址都不是真的, 你写的C程序里%p打印出来的都是虚拟地址. shell跑两个进程, 打印出来的地址可能是相同的. 而实际上在物理内存上两个进程的地址空间肯定不同.

实际上，作为用户级程序的程序员，可以看到的任何地址都是虚拟地址。只有操作系统，通过精妙的虚拟化内存技术，知道这些指令和数据所在的物理内存的位置。所以永远不要忘记：如果你在一个程序中打印出一个地址，那就是一个虚拟的地址。虚拟地址只是提供地址如何在内存中分布的假象，只有操作系统（和硬件）才知道物理地址。 
虚拟地址由操作系统和硬件翻译成物理地址，以便从真实的物理位置获取该地址的值。

### 13.5 小结

虚拟内存系统负责为程序提供一个巨大的、稀疏的、私有的地址空间的假象，其中保存了程序的所有指令和数据。操作系统在专门硬件的帮助下，通过每一个虚拟内存的索引，将其转换为物理地址，物理内存根据获得的物理地址但获取所需的信息。操作系统会同时对许多进程执行此操作，并且确保程序之间互相不会受到影响，也不会影响操作系统。整个方法需要大量的机制（很多底层机制）和一些关键的策略。


## Ch14 插叙: 内存操作API

介绍 UNIX 操作系统的内存分配接口. 关键问题：如何分配和管理内存
在 UNIX/C 程序中，理解如何分配和管理内存是构建健壮和可靠软件的重要基础。通常使用哪些接口？哪些错误需要避免?

### 14.1 内存类型

在运行一个 C 程序的时候, 会分配两种类型的内存. 第一种称为`栈内存`, 它的申请和释放操作是编译器来隐式管理的, 所以有时也称为`自动(automatic)内存`. 
你在函数内声明个变量, 编译器在你进入该函数时, 在栈上给你分配个空间. 函数运行结束, 这个栈空间就随着栈帧被释放掉了. 那如果你希望某些信息存在于函数调用之外，就不要放在栈上。

第二种类型的内存，即所谓的堆（heap）内存，其中所有的申请和释放操作都由程序员显式地完成。它不会被自动释放掉, 所以要注意.

### 14.2 malloc()调用

这个是用户态库函数. 在头文件 stdlib.h中有声明. 实际上, 甚至都不需这样做, 因为 C 库是 C 程序默认链接的, 其中就有`mallock()`的代码，加上这个头文件只是让编译器检查你是否正确调用了 malloc()（即传入参数的数目正确且类型正确）。
不建议直接传入一个数字作为参数, 可以使用sizeof或者宏. 这个大小, 编译时就已经知道了.
注意sizeof(一个指针变量名)的情况, 跟sizeof(一个数组变量名)的区别.
关于字符串, 有个习惯用法: malloc(strlen(s) + 1). 这个是跟sizeof(指针变量名)是有关的, 因为这样拿不到目标字符串的长度的. 而strlen却可以拿到除了`'\0'`的字符串长度.
malloc()返回一个指向 void 类型的指针, 这样做只是 C 中传回地址的方式，让程序员决定如何处理它. 程序员将进一步使用所谓的强制类型转换(cast).

### 14.3 free()调用

分配内存很简单. 难在知道何时、如何以及是否释放内存. 不释放就会内存泄露.

### 14.4 常见错误

面向对象语言比如java, 会有自己的内存管理, 会有垃圾收集器会帮你自动释放.

- 忘记分配内存

比如你要把字符串复制到一个指针指向的内存空间, 而你却忘了申请这个空间. 可以用 strdup()替你分配, 并完成复制操作咯.

- 没有分配足够的内存

没有分配足够的内存, 就会缓冲区溢出(buffer overflow).
溢出可能具有很大的危害，实际上是系统中许多安全漏洞的来源. 在有些情况下，malloc 库总是分配一些额外的空间，因此你的程序实际上溢出时不会踩到其他变量. 还有一些情况下，该程序确实会发生故障和崩溃.

- 忘记初始化分配的内存

读取未初始化的内存数据, 进行使用是很危险的.

- 忘记释放内存

虽然申请的内存是在进程内, 进程跑完了, 就会回收. 但这是个坏习惯. 程序跑着跑着, 怎么就没有内存了呢.

- 在用完之前释放内存

内存还在用, 就释放了. 基本会崩.

- 反复释放内存

重复释放内存(double free). 内存分配库可能会感到困惑，并且会做各种奇怪的事情，崩溃是常见的结果。

- free()参数错误

系统中实际存在两级内存管理, 第一级是由操作系统执行的内存管理，操作系统在进程运行时将内存交给进程，并在进程退出（或以其他方式结束）时将其回收。
第二级管理在每个进程中，例如在调用 malloc()和 free()时，在堆内管理。即使你没有调用 free()（并因此泄露了堆中的内存），操作系统也会在程序结束运行时，收回进程的所有
内存（包括用于代码、栈，以及相关堆的内存页）。无论地址空间中堆的状态如何，操作系统都会在进程终止时收回所有这些页面，从而确保即使没有释放内存，也不会丢失内存。

如果你的进程任务很短, 不free也会被OS回收. 但你的进程是常驻的进程, 那内存泄露可要了命了.

由于内存出错很常见，整个工具生态圈已经开发出来，可以帮助你在代码中找到这些问题。比如purify和valgrind. 这是两个很好用的内存问题排查的工具.

### 14.5 底层操作系统支持

malloc()和 free() 不是系统调用, 而是库调用. malloc 库管理虚拟地址空间内的空间，但是它本身是建立在一些系统调用之上的，这些系统调用会进入操作系统，来请求本多内存或者将一些内容释放回系统。

有一个系统调用叫 brk, 它被用来改变程序分断（break）的位置, 也就是堆尾(结束)的位置.
它需要一个参数(新break的地址), 从而根据新break是大于还是小于当前break, 来增加或减
小堆空间的大小. 另一个调用sbrk的参数是传入一个增量, 但目的是类似的.
一般写程序不会用这两个系统调用的.

还可以通过 mmap()调用从操作系统获取内存, 通过传入正确的参数，mmap()可以在程序中创建一个匿名（anonymous）内存区域——这个区域不与任何特定文件相关联，而是与交换空间（swap space）相关联，稍后我们将在虚拟内存中详细讨论。这种内存也可以像堆一样对待并管理。

### 14.6 其他调用

calloc: 自动初始化分配的内存.
realloc:  若创建一个新的更大的内存区域，则将旧区域复制到其中，并返回新区域的指针。

## Ch15 机制: 地址转换

为了实现高效的虚拟化，操作系统应该尽量让程序自己运行，同时通过在关键点的及时介入（interposing），来保持对硬件的控制。高效和控制是现代操作系统的两个主要目标。
高效决定了我们要利用硬件的支持, 这在开始的时候非常初级（如使用一些寄存器），但会变得相当复杂（比如我们会讲到的 TLB、页表等）。
控制意味着操作系统要确保应用程序只能访问它自己的内存空间. 因此，要保护应用程序不会相互影响，也不会影响操作系统，我们需要硬件的帮助。
最后，我们对虚拟内存还有一点要求，即灵活性.具体来说，我们希望程序能以任何方式访问它自己的地址空间，从而让系统更容易编程。

关键问题：如何高效、灵活地虚拟化内存
如何实现高效的内存虚拟化？如何提供应用程序所需的灵活性？如何保持控制应用程序可访问的内存位置，从而确保应用程序的内存访问受到合理的限制？如何高效地实现这一切？

这里会用一种通用技术, 有时被称为基于硬件的地址转换(hardware-based address translation), 简称为地址转换（address translation）。
它可以看成是`受限直接执行`这种一般方法的补充。
利用地址转换，硬件对每次内存访问进行处理（即指令获取、数据读取或写入），将指令中的虚拟（virtual）地址转换为数据实际存储的物理（physical）地址。
在每次内存引用时，硬件都会进行地址转换，将应用程序的内存引用重定位到内存中实际的位置。

仅仅依靠硬件不足以实现虚拟内存，因为它只是提供了底层机制来提高效率。操作系统必须在关键的位置介入，设置好硬件，以便完成正确的地址转换。因此它必须管理内存（manage memory），记录被占用和空闲的内存位置，并明智而谨慎地介入，保持对内存使用的控制。

虚拟现实的背后是丑陋的物理事实：许多程序其实是在同一时间共享着内存，就像 CPU（或多个 CPU）在不同的程序间切换运行。
通过虚拟化，操作系统（在硬件的帮助下）将丑陋的机器现实转化成一种有用的、强大的、易于使用的抽象。

### 15.1 假设

我们先假设用户的虚拟地址空间必须连续地放在物理内存中。同时，为了简单，我们假设虚拟地址空间不是很大，具体来说，小于物理内存的大小。最后，假设每个地址空间的大小完全一样。
后面会逐步打破这些假设.

### 15.2 一个例子

为了更好地理解实现地址转换需要什么，以及为什么需要，我们先来看一个简单的例子。

- ![](assets/Pasted%20image%2020230328172637.png)

设想一个进程的地址空间如图 15.1 所示。
这里我们要检查一小段代码，它从内存中加载一个值，对它加 3，然后将它存回内存。你可以设想，这段代码的 C 语言形式可能像这样：

```c
void func() {
	int x;
	x = x + 3; 
```
编译器将这行代码转化为汇编语句，可能像下面这样（x86 汇编）。我们可以用 Linux的 objdump 或者 Mac 的 otool 将它反汇编：
```asm
128: movl 0x0(%ebx), %eax ;load 0+ebx into eax
132: addl $0x03, %eax ;add 3 to eax register
135: movl %eax, 0x0(%ebx) ;store eax back to mem
```
这段代码相对简单，它假定 x 的地址已经存入寄存器 ebx，之后通过 movl 指令将这个地址的值加载到通用寄存器 eax（长字移动）。下一条指令对 eax 的内容加 3。最后一条指令将 eax 中的值写回到内存的同一位置。

> 介入是一种很常见又很有用的技术，计算机系统中使用介入常常能带来很好的效果。在虚拟内存中，硬件可以介入到每次内存访问中，将进程提供的虚拟地址转换为数据实际存储的物理地址。但是，一般化的介入技术有更广阔的应用空间，实际上几乎所有良好定义的接口都应该提供功能介入机制，以便增加功能或者在其他方面提升系统。这种方式最基本的优点是透明（transparency），介入完成时通常不需要改动接口的客户端，因此客户端不需要任何改动。

在图 15.1 中，可以看到代码和数据都位于进程的地址空间，3 条指令序列位于地址 128（靠近头部的代码段），变量 x 的值位于地址 15KB（在靠近底部的栈中）。如图 15.1 所示，x的初始值是 3000。

如果这 3 条指令执行，从进程的角度来看，发生了以下几次内存访问：
- 从地址 128 获取指令；
- 执行指令（从地址 15KB 加载数据）；
- 从地址 132 获取命令；
- 执行命令（没有内存访问）；
- 从地址 135 获取指令；
- 执行指令（新值存入地址 15KB）。

从`程序的角度`来看，它的地址空间（address space）从 0 开始到 16KB 结束。它包含的所有内存引用都应该在这个范围内。然而，对`虚拟内存`来说，操作系统希望将这个进程地址空间放在`物理内存的其他位置`，并不一定从地址 0 开始。
因此我们遇到了如下问题：怎样在内存中重定位这个进程，同时对该进程透(transparent)？怎么样提供一种虚拟地址空间从 0 开始的假象，而实际上地址空间位于另外某个物理地址？

- ![](assets/Pasted%20image%2020230328173557.png)

图15.2展示了一个例子, 说明这个进程的地址空间被放入物理内存后可能的样子. 从图15.2中可以看到, 操作系统将第一块物理内存留给了自己, 并将上述例子中的`进程地址空间重定位`到从32KB开始的物理内存地址. 剩下的两块内存空闲(16～32KB 和 48～64KB). 

### 15.3 动态(基于硬件)重定位

为了更好地理解`基于硬件的地址转换`, 先看看其第一次应用. 1950s后期, 首次出现, 思想就是: 基址加界限机制(base and bound). 又称为动态重定位(dynamic relocation).

具体来说，每个 CPU 需要两个硬件寄存器: 基址(base)寄存器和界限(bound)寄存器, 有时称为限制(limit)寄存器.
这组基址和界限寄存器，让我们能够将地址空间放在物理内存的任何位置，同时又能确保进程只能访问自己的地址空间.

采用这种方式, 在`编写和编译`程序时`假设地址空间从零开始`. 但是，当程序真正执行时，`操作系统会决定`其在`物理内存`中的`实际加载`地址，并将起始地址`记录`在`基址寄存器`中.
比如图15.2, 就可以把32K地址, 放到基址寄存器中, 就确定了进程的起始地址.

当进程运行时，有趣的事情发生了. 现在，该进程产生的所有内存引用，都会被处理器通过以下方式转换为物理地址: 
physical address = virtual address + base

进程中使用的内存引用都是虚拟地址(virtual address), `硬件`接下来将虚拟地址加上基址寄存器中的内容. 得到物理地址(physical address), 再通过地址总线发给内存系统. 

为了更好地理解，让我们追踪一条指令执行的情况。具体来看前面序列中的一条指令：
128: movl 0x0(%ebx), %eax

程序计数器(PC)首先被设置为128. 当硬件需要获取这条指令时, 它先将这个值加上基址寄存器中的 32KB(32768), 得到实际的物理地址32896, 然后硬件从这个物理地址获取指令.
取完指令, 指令译码，然后处理器开始执行该指令。这时，进程发起从虚拟地址15KB去取数据, 处理器同样将虚拟地址加上基址寄存器内容(32KB), 得到最终的物理地址47KB, 从而获得需要的数据. 

将`虚拟地址`转换为`物理地址`, 这正是所谓的`地址转换(address translation)技术`.
也就是说，硬件取得进程认为自己要访问的地址，将此地址转换成数据实际位于的物理地址。
由于这种重定位是在运行时发生的, 而且我们甚至可以在进程开始运行后改变其地址空间, 这种
技术一般被称为动态重定位(dynamic relocation).

> 在动态重定位的过程中，只有很少的硬件参与，但获得了很好的效果。一个基址寄存器将虚拟地址转换为物理地址，一个界限寄存器确保这个地址在进程地址空间的范围内。它们一起提供了既简单又高效的虚拟内存机制。

界限寄存器提供了访问保护. 图15.2的例子, 界限寄存器里的值就是16KB. 如果进程需要访问超过这个界限或者为负数的虚拟地址，CPU 将触发异常，进程最终可能被终止。
界限寄存器的用处在于, 它确保了进程产生的所有地址都在进程的地址“界限”中.

这种基址寄存器配合界限寄存器的硬件结构是`芯片`中的（每个 CPU 一对）。有时我们将CPU 的这个负责地址转换的部分统称为`内存管理单元`（Memory Management Unit，MMU）。随着我们开发更复杂的内存管理技术，MMU 也将有更复杂的电路和功能。

关于`界限寄存器`再补充一点, 它通常有两种使用方式. 
在一种方式中(像上面那样), 它记录地址空间的大小, 硬件在将虚拟地址与基址寄存器内容求和前, 就检查这个界限. 
另一种方式是界限寄存器中记录地址空间结束的物理地址, 硬件在转化虚拟地址到物理地址之后才去检查这个界限. 这两种方式在逻辑上是等价的. 简单起见, 我们这里假设采用第一种方式. 

- 转换示例

设想一个进程拥有 4KB 大小地址空间，它被加载到从 16KB 开始的物理内存中。一些地址转换结果见表 15.1。

- ![](assets/Pasted%20image%2020230328184231.png)

虚拟地址其实就是偏移量.

>补充：数据结构——空闲列表
>操作系统必须记录哪些空闲内存没有使用，以便能够为进程分配内存。很多不同的数据结构可以用于这项任务，其中最简单的（也是我们假定在这里采用的）是空闲列表（free list）。它就是一个列表，记录当前没有使用的物理内存的范围.

### 15.4 硬件支持: 总结

总结一下需要的硬件支持(见表 15.2).
首先, 正如在CPU虚拟化的章节中提到的, 我们需要两种CPU模式. 操作系统在特权模式(privileged mode，或内核模式，kernelmode), 可以访问整个机器资源.
应用程序在用户模式（user mode）运行，只能做有限的操作。只要一个位，也许保存在处理器状态字（processor status word）中，就能说明当前的CPU 运行模式。
在一些特殊的时刻（如系统调用、异常或中断），CPU 会切换状态。

- ![](assets/Pasted%20image%2020230328185845.png)

硬件还必须提供基址和界限寄存器(base and bounds register), 因此每个CPU的内存管理单元(Memory Management Unit，MMU)都需要这两个额外的寄存器.
用户程序的虚拟地址, 要转为物理地址, 硬件也必须能检查物理地址是否有效，通过界限寄存器和CPU 内的一些电路来实现。

硬件应该提供一些特殊的指令，用于修改基址寄存器和界限寄存器，允许操作系统在切换进程时改变它们。这些指令是特权（privileged）指令，只有在内核模式下，才能修改这些寄存器。

在用户程序尝试非法访问内存（越界访问）时，CPU必须能够产生异常（exception）。
在这种情况下，CPU 应该阻止用户程序的执行，并安排操作系统的“越界”异常处理程（exception handler）去处理。操作系统的处理程序会做出正确的响应，比如在这种情况下终止进程。类似地，如果用户程序尝试修改基址或者界限寄存器时，CPU 也应该产生异常.
CPU 还必须提供一种方法，来通知它这些处理程序的位置，因此又需要另一些特权指令。

### 15.5 操作系统的问题

为了支持动态重定位，硬件添加了新的功能，使得操作系统有了一些必须处理的新问题.
硬件支持和操作系统管理结合在一起，实现了一个简单的虚拟内存。具体来说，在一些关键的时刻操作系统需要介入，以实现基址和界限方式的虚拟内存.

- ![](assets/Pasted%20image%2020230328191038.png)

第一, 在进程创建时, 操作系统必须采取行动, 为进程的地址空间找到内存空间.
由于我们假设每个进程的地址空间小于物理内存的大小, 并且大小相同, 这对操作系统来说简单.
它可以把整个物理内存看作一组槽块, 标记了空闲或已用.
当新进程创建时，操作系统检索这个数据结构（常被称为空闲列表，free list)，为新地址空间找到位置，并将其标记为已用。如果地址空间可变，那就复杂了，后续章节中讨论.

看一个例子, 在图15.2中, 操作系统将物理内存的第一个槽块分配给自己, 然后将例子中的进程重定位到物理内存地址32KB. 另两个槽块(16～32KB, 48～64KB)空闲, 因此空闲列表(free list)就包含这两个槽块.

第二, 在进程终止时(正常退出，或因行为不端被强制终止), 操作系统也必须做一些工作，回收它的所有内存, 给其他进程或者操作系统使用. 在进程终止时, 操作系统会将这些内存放回到空闲列表, 并根据需要清除相关的数据结构. 

第三，在上下文切换时，操作系统也必须执行一些额外的操作。
每个CPU毕竟只有一个基址寄存器和一个界限寄存器, 但对于每个运行的进程, 它们的值都不同, 因为每个程序被加载到内存中不同的物理地址.
因此, 在切换进程时, 操作系统必须保存和恢复基础和界限寄存器. 
具体来说, 当操作系统决定中止当前的运行进程时, 它必须将当前基址和界限寄存器中的内容保存在内存中, 放在某种每个进程都有的结构中, 如进程结构(process structure)或进程控制块(Process Control Block, PCB)中。
当操作系统恢复执行某个进程时(或第一次执行), 也必须给基址和界限寄存器设置正确的值.

注意, 当进程暂停时(即没有运行), 操作系统可以改变其地址空间的物理位置, 这很容易. 
要移动进程的地址空间, 操作系统首先让进程停止运行, 然后将地址空间拷贝到新位置, 最后更新保存的基址寄存器(在进程结构中), 指向新位置.
当该进程恢复执行时, 它的(新)基址寄存器会被恢复, 它再次开始运行, 显然它的指令和数据都在新的内存位置了.

第四, 操作系统必须提供异常处理程序(exception handler), 或要一些调用的函数, 像上面提到的那样。操作系统在启动时加载这些处理程序(通过特权命令). 
例如, 当一个进程试图越界访问内存时, CPU 会触发异常. 在这种异常产生时, 操作系统必须准备采取行动. 通常操作系统会做出充满敌意的反应: 终止错误进程.  

表 15.4 为按时间线展示了大多数硬件与操作系统的交互。
注意，地址转换过程完全由硬件处理，没有操作系统的介入。
在这个时候，发生时钟中断，操作系统切换到进程 B 运行，它执行了“错误的加载”（对一个非法内存地址），这时操作系统必须介入，终止该进程，清理并释放进程 B 占用的内存，将它从进程表中移除。
从表中可以看出，我们仍然遵循受限直接访问（limited direct execution）的基本方法，大多数情况下，操作系统正确设置硬件后，就任凭进程直接运行在 CPU 上，只有进程行为不端时才介入。

- ![](assets/Pasted%20image%2020230328195909.png)

- ![](assets/Pasted%20image%2020230328195921.png)

### 15.6 小结

本章通过虚拟内存使用的一种特殊机制，即地址转换（address translation），扩展了受限直接访问的概念。
利用地址转换，操作系统可以控制进程的所有内存访问，确保访问在地址空间的界限内.
这个技术高效的关键是硬件支持，硬件快速地将所有内存访问操作中的虚拟地址（进程自己看到的内存位置）转换为物理地址（实际位置）.
所有的这一切对进程来说都是透明的，进程并不知道自己使用的内存引用已经被重定位.

了解了一种特殊的虚拟化方式，称为基址加界限的动态重定位.
这种方式非常高效，因为只需要很少的硬件逻辑, 就可以将虚拟地址和基址寄存器加起来，并检查进程产生的地址没有越界。
基址加界限也提供了保护，操作系统和硬件的协作，确保没有进程能够访问其地址空间之外的内容。

遗憾的是，这个简单的动态重定位技术有效率低下的问题.
例如，从图 15.2 中可以看到，重定位的进程使用了从 32KB 到 48KB 的物理内存，但由于该进程的栈区和堆区并不很大，导致这块内存区域中大量的空间被浪费.
这种浪费通常称为内部碎片（internal fragmentation），指的是已经分配的内存单元内部有未使用的空间（即碎片），造成了浪费。
(在我们当前的方式中，即使有足够的物理内存容纳更多进程，但我们目前要求将地址空间放在固定大小的槽块中，因此会出现内部碎片).
所以，我们需要更复杂的机制，以便更好地利用物理内存，避免内部碎片。

本章的第一次尝试是将基址加界限的概念稍稍泛化，得到`分段(segmentation)`的概念，我们接下来将讨论.

## Ch16 分段

到目前为止，我们都是假设将所有进程的地址空间完整地加载到内存中。利用基址和界限寄存器，操作系统很容易将不同进程`重定位`到不同的物理内存区域. 
但是，对于这些内存区域，你可能已经注意到：栈和堆之间，有一大块“空闲”空间。

- ![](assets/Pasted%20image%2020230329043554.png)

从图 16.1 中可知，如果我们将整个地址空间放入物理内存，那么栈和堆之间的空间并没有被进程使用，却依然占用了实际的物理内存。
因此, 简单的通过基址寄存器和界限寄存器实现的虚拟内存很浪费.
而且, 如果剩余物理内存无法提供连续区域来放置完整的地址空间，别的进程便无法运行.
这种基址加界限的方式看来并不像我们期望的那样灵活。

关键问题：怎样支持大地址空间
怎样支持大地址空间，同时栈和堆之间(可能)有大量空闲空间? 在之前的例子里，地址空间非常小，所以这种浪费并不明显。但设想一个 32 位(4GB)的地址空间，通常的程序只会使用几兆的内存，但需要整个地址空间都放在内存中。

### 16.1 分段：泛化的基址/界限

为了解决这个问题, 分段(segmentation)的概念应运而生. 
这个想法很简单, 在MMU中引入不止一对基址和界限寄存器, 而是给地址空间内的每个逻辑段(segment)分配一对.
一个段只是地址空间里的一个连续定长的区域, 在典型的地址空间里有3个逻辑不同的段: 代码, 栈和堆(还有个数据段). 
分段的机制使得操作系统能够将不同的段放到不同的物理内存区域, 从而避免了虚拟地址空间中的未使用部分占用物理内存. 

假设我们希望将图 16.1 中的地址空间放入物理内存。通过给每个段一对基址和界限寄存器，可以将每个段独立地放入物理内存。如图 16.2 所示，64KB 的物理内存中放置了 3 个段（为操作系统保留 16KB）。
从图中可以看到，只有已用的内存才在物理内存中分配空间，因此可以容纳巨大的地址空间，其中包含大量未使用的地址空间(有时又称为`稀疏地址空间`, sparse address spaces).

那就需要 MMU 中的硬件结构来支持分断：在这种情况下，需要一组3对基址和界限寄存器。表 16.1 展示了上面的例子中的寄存器值，每个界限寄存器记录了一个段的大小。 

- ![](assets/Pasted%20image%2020230329045743.png)

>补充：段错误
>段错误指的是在支持分段的机器上发生了非法的内存访问。有趣的是，即使在不支持分段的机器上这个术语依然保留。但如果你弄不清楚为什么代码老是出错，就没那么有趣了。

访问非法地址, 未分配给本进程的地址, 去访问就会报段错误.

### 16.2 我们引用哪个段

硬件在地址转换时使用`段寄存器`. 它如何知道段内的偏移量, 以及地址引用了`哪个段`?

一种常见的方式，有时称为`显式(explicit)方式`，就是用虚拟地址的开头几位来标识不同的段, VAX/VMS 系统使用了这种技术.
在我们之前的例子中, 有3个段, 因此需要两位来标识. 如果我们用 14 位虚拟地址的前两位来标识，那么虚拟地址如下所示：

- ![](assets/Pasted%20image%2020230329051039.png)

如果前两位是 00，硬件就知道这是属于代码段的地址，因此使用代码段的基址和界限来重定位到正确的物理地址。如果前两位是 01，则是堆地址，对应地，使用堆的基址和界限。

下面来看一个 4200 之上的堆虚拟地址，进行进制转换，确保弄清楚这些内容。虚拟地址 4200 的二进制形式如下：

- ![](assets/Pasted%20image%2020230329052238.png)

01表示是堆段, 剩下12位是段内偏移量,  34K+104 就是物理地址.
注意，偏移量也简化了对段边界的判断。我们只要检查偏移量是否小于界限，大于界限的为非法地址。
因此，如果基址和界限放在数组中（每个段一项)，为了获得需要的物理地址，硬件会做下面这样的事：

```c
1 // get top 2 bits of 14-bit VA
2 Segment = (VirtualAddress & SEG_MASK) >> SEG_SHIFT
3 // now get offset
4 Offset = VirtualAddress & OFFSET_MASK
5 if (Offset >= Bounds[Segment])
6     RaiseException(PROTECTION_FAULT)
7 else
8     PhysAddr = Base[Segment] + Offset
9     Register = AccessMemory(PhysAddr)
```
在我们的例子中，可以为上面的常量填上值。具体来说，SEG_MASK 为 0x3000，SEG_SHIFT 为 12，OFFSET_MASK 为 0xFFF。

上面使用两位来区分段，但实际只有 3 个段（代码、堆、栈），因此有一个段的地址空间被浪费。因此有些系统中会将堆和栈当作同一个段，因此只需要一位来做标识.

硬件还有其他方法来决定特定地址在哪个段. 
在`隐式(implicit)方式`中，硬件通过地址产生的方式来确定段。
例如，如果地址由程序计数器产生(即它是指令获取), 那么地址在代码段. 如果基于栈或基址指针, 它一定在栈段. 其他地址则在堆段.

### 16.3 栈怎么办

到目前为止，我们一直没有讲地址空间中的一个重要部分：栈. 
在表 16.1 中，栈被重定位到物理地址 28KB。但有一点关键区别，它反向增长。在物理内存中，它始于 28KB，增长回到 26KB，相应虚拟地址从 16KB 到 14KB。地址转换必须有所不同。

首先，我们需要一点硬件支持. 除了基址和界限外，硬件还需要知道段的增长方向（用一位区分，比如 1 代表自小而大增长，0 反之）.

- ![](assets/Pasted%20image%2020230329053211.png)

硬件理解段可以反向增长后，这种虚拟地址的地址转换跟之前的不一样了. 举例:

假设要访问虚拟地址 15KB，它应该映射到物理地址 27KB。该虚拟地址的二进制形式是：111100 0000 0000（十六进制 0x3C00）。硬件利用前两位（11）来指定段，但然后我们要处理偏移量 3KB。为了得到正确的反向偏移，我们必须从 3KB 中减去最大的段地址：在这个例子中，段可以是 4KB，因此正确的偏移量是 3KB 减去 4KB，即−1KB。只要用这个反向偏移量（−1KB）加上基址（28KB），就得到了正确的物理地址 27KB。用户可以进行界限检查，确保反向偏移量的绝对值小于段的大小。

### 16.4 支持共享

随着分段机制的不断改进, 系统设计人员很快意识到, 再多一点的硬件支持, 就能实现新的效率提升. 具体来说，要节省内存，有时候可以在地址空间之间`共享(share)`某些内存段. 
代码共享很常见, 今天的系统仍然在使用.

为了支持共享, 需要一些额外的硬件支持, 这就是保护位(protection bit).
基本为每个段增加了几个位, 标识程序是否能够读写该段, 或执行其中的代码. 
通过将`代码段`标记为`只读`, 同样的代码可以被多个进程共享, 而不用担心破坏隔离. 
虽然每个进程都认为自己独占这块内存, 但操作系统秘密地共享了内存, 进程`不能修改`这些内存, 所以假象得以保持.

表 16.3 展示了一个例子，是硬件（和操作系统）记录的额外信息。可以看到，代码段的权限是可读和可执行，因此物理内存中的一个段可以映射到多个虚拟地址空间。

- ![](assets/Pasted%20image%2020230329054741.png)

有了保护位，前面描述的硬件算法也必须改变。除了检查虚拟地址是否`越界`，硬件还需要检查特定访问是否`允许`。如果用户进程试图写入只读段，或从非执行段执行指令，硬件会触发`异常`，让操作系统来处理出错进程。

### 16.5 细粒度与粗粒度的分段

到目前为止, 我们的例子大多针对只有很少的几个段的系统(即代码、栈、堆).
我们可以认为这种分段是`粗粒度的(coarse-grained)`, 因为它将地址空间分成较大的, 粗粒度的块.
而早期系统, 更灵活, 允许将地址空间划分为大量较小的段, 这被称为`细粒度(fine-grained)分段`.

支持许多段需要进一步的`硬件支持`, 并在`内存中`保存某种`段表(segment table)`.
这种段表通常支持创建非常多的段，因此系统使用段的方式，可以比之前讨论的方式更灵活。
有了操作系统和硬件的支持，编译器可以将代码段和数据段划分为许多不同的部分.
当时的考虑是，通过更细粒度的段，操作系统可以更好地了解哪些段在使用哪些没有，从而可以更高效地利用内存。

### 16.6 操作系统支持

系统运行时, 虚拟地址空间中的不同段被重定位到物理内存中. 
与最初介绍的整个进程地址空间只有一个基址/界限寄存器对的方式相比, 大量节省了物理内存.
具体来说，栈和堆之间没有使用的区域就不需要再分配物理内存，让我们能将更多地址空间放进物理内存。

然而, 分段也带来了一些新的问题. 先介绍必须关注的操作系统新问题. 
第一个是老问题：操作系统在上下文切换时应该做什么？
各个`段寄存器`中的内容必须`保存和恢复`。显然，每个进程都有自己独立的虚拟地址空间，操作系统必须在进程运行前，确保这些寄存器被正确地赋值。

第二个问题更重要，即管理`物理内存的空闲空间`. 
新的地址空间被创建时，操作系统需要在物理内存中为它的段找到空间。
每个进程都有一些段，每个段的大小也可能是不同的。怎么办?

一般会遇到的问题是, 物理内存很快充满了许多空闲空间的小洞, 不连续因而很难分配给新的段, 或扩大已有的段. 这种问题被称为`外部碎片(external fragmentation).` 如图 16.3（左边）所示。

- ![](assets/Pasted%20image%2020230329064214.png)

在这个例子中, 一个进程需要分配一个 20KB 的段. 当前有 24KB 空闲, 但并不连续(是3 个不连续的块). 因此, 操作系统无法满足这个 20KB 的请求.

该问题的一种解决方案是紧凑（compact）物理内存，重新安排原有的段。
例如，操作系统先终止运行的进程，将它们的数据复制到连续的内存区域中去，改变它们的段寄存器中的值，指向新的物理地址，从而得到了足够大的连续空闲空间。
然而内存紧凑成本很高，因为拷贝段是内存密集型的，一般会占用大量的处理器时间。图 16.3（右边）是紧凑后的物理内存。

一种更简单的做法是利用`空闲列表管理算法`，试图保留大的内存块用于分配。
相关的算法可能有成百上千种，包括传统的最优匹配（best-fit，从空闲链表中找最接近需要分配空间的空闲块返回）、最坏匹配（worst-fit）、首次匹配（first-fit）以及像`伙伴算法`（buddy algorithm）这样更复杂的算法.

然而, 无论算法多么精妙, 都无法完全消除外部碎片, 因此, 好的算法只是试图减小它.
存在这么多不同的算法来尝试减少外部碎片, 正说明没有最好的方案来解决这个问题. 因此只能找到一个合理的方案. 唯一真正的解决办法就是(会在后面看到), 完全避免这个问题, 永远不要分配不同大小的内存块.

### 16.7 小结

从将所有进程地址空间, 都整个地加载到内存, 用基址和界限方法造成大量内存浪费. 剩余地址不够新进程用的. 引出了问题, 怎么支持大地址空间.

从而进出分段的概念, 进程有多个段(代码, 数据, 堆, 栈), 那由原来的一组基址和界限寄存器, 扩展到每个段一对基址, 界限寄存器.  这就可以让不同的段, 加载到不同的物理内存. 从而避免了虚拟地址空间中的未使用部分占用物理内存. 

多个段引出了新问题, 系统怎么知道要访问的是哪个段呢.
显式方式, 用虚拟地址高位的n位来标识不同段, 剩下的数据位, 标识段内的偏移量.
隐式方式, 硬件通过地址产生的方式来确定段. 若地址由PC产生, 那地址是在代码段. 若基于栈或基址指针, 一定在栈段. 其他地址在堆栈.

由于栈比较独特, 增长方向跟别人相反. 引出一个硬件的寄存器数据位, 来区分正反向增长.

要想进一步提升效率, 需要更多的硬件支持. `地址空间共享内存段`, 就可以节省内存. 需要的硬件支持, 就是保护位. 每个段增加几个位, 作为保护位, 标识程序是否能够读写该段,  或执行其中的代码.
`代码段`被标记为`只读`, 同样的代码可以被多个进程共享. 而不用担心破坏隔离. 
物理内存中的代码段就可以映射到多个虚拟地址空间.

上面这种都是粗粒度的分段. 还有细粒度的分段. 实现细粒度分段, 需要进一步的硬件支持, 并在内存中保存段表.

分段后, 操作系统切换上下文的时候, 各个段寄存器要保存, 恢复. 
还有要管理物理内存的空闲空间. 问题在于分段, 会造成外部碎片, 使空闲的内存不连续, 从而无法分配给新进程.

解决方案是紧凑 物理内存, 重新安排原有的段. 这种方法开销很大.
另一种是用空闲列表管理算法. 试图保留大的内存块用于分配. 后面会介绍.
但, 无论算法多么精妙, 都无法完全消除外部碎片, 因此, 好的算法只是试图减小它。

> 分段解决了一些问题，帮助我们实现了更高效的虚拟内存。不只是动态重定位，通过避免地址空间的逻辑段之间的大量潜在的内存浪费，分段能更好地支持稀疏地址空间。它还很快，因为分段要求的算法很容易，很适合硬件完成，地址转换的开销极小。分段还有一个附加的好处：代码共享。如果代码放在独立的段中，这样的段就可能被多个运行的程序共享。

> 但我们已经知道，在内存中分配不同大小的段会导致一些问题，我们希望克服。首先，是我们上面讨论的外部碎片。由于段的大小不同，空闲内存被割裂成各种奇怪的大小，因此满足内存分配请求可能会很难。用户可以尝试采用聪明的算法，或定期紧凑内存，但问题很根本，难以避免。

> 第二个问题也许更重要，分段还是不足以支持更一般化的稀疏地址空间。例如，如果有一个很大但是稀疏的堆，都在一个逻辑段中，整个堆仍然必须完整地加载到内存中。换言之，如果使用地址空间的方式不能很好地匹配底层分段的设计目标，分段就不能很好地工作。因此我们需要找到新的解决方案。

## Ch17 空闲空间管理

前一章提到了, 内存碎片的问题. 暂且将对虚拟内存的讨论放在一边, 来讨论所有内存管理系统的一个基本方面，无论是 malloc 库(管理进程中堆的页)，还是操作系统本身(管理进程的地址空间). 具体来说，我们会讨论空闲空间管理(free-space management)的一些问题。

问题更明确一点. 管理空闲空间当然可以很容易, 我们会在讨论分页概念时看到. 如果需要管理的空间被划分为固定大小的单元, 就很容易. 在这种情况下, 只需要维护这些大小固定的单元的列表, 如果有请求, 就返回列表中的第一项.

如果要管理的空闲空间由`大小不同的单元`构成，管理就变得困难. 这种情况出现在用户级的内存分配库(如 malloc()和 free()), 或者操作系统用分段(segmentation)的方式实现虚拟内存. 
在这两种情况下, 出现了外部碎片(external fragmentation)的问题: 空闲空间被分割成不同大小的小块, 成为碎片, 后续的请求可能失败, 因为没有一块足够大的连续空闲空间, 即使这时总的空闲空间超出了请求的大小. 

关键问题：如何管理空闲空间
要满足变长的分配请求, 应该如何管理空闲空间? 什么策略可以让碎片最小化? 不同方法的时间和空间开销如何?

### 17.1 假设

本章的大多数讨论，将聚焦于`用户级内存分配库`中的分配程序.

`假定`基本的接口就像 malloc()和 free()提供的那样. 具体来说，void * malloc(size tsize)需要一个参数 size, 它是应用程序请求的字节数. 函数返回一个指针(没有具体的类型, 在 C 语言是void 类型), 指向这样大小(或较大一点)的一块空间.
对应的函数void free(void \*ptr)函数接受一个指针，释放对应的内存块。请注意该接口的隐含意义, 在释放空间时, 用户不需告知库这块空间的大小. 因此, 在只传入一个指针的情况下, 库必须能够弄清楚这块内存的大小. 

该库管理的空间由于历史原因被称为堆, 在堆上`管理空闲空间的数据结构`通常称为`空闲列表(free list)`. 该结构包含了管理内存区域中`所有空闲块的引用`. 当然, 该数据结构不一定真的是列表, 而只是`某种可以追踪空闲空间的数据结构`. 

进一步`假设`, 我们主要关心的是外部碎片(external fragmentation), 分配程序也可能有内部碎片(internal fragmentation)的问题.
如果`分配程序给出的内存块超出请求的大小`, 在这种块中超出请求的空间(因此而未使用)就被认为是`内部碎片`(因为浪费发生在已分配单元的内部, 这是另一种形式的空间浪费.)

我们还`假设`，内存一旦被分配给客户，就不可以被重定位到其他位置。
例如，一个程序调用 malloc()，并获得一个指向堆中一块空间的指针，这块区域就“属于”这个程序了，库不再能够移动，直到程序调用相应的 free()函数将它归还。
因此，不可能进行紧凑（compaction）空闲空间的操作，从而减少碎片. 但是，操作系统层在实现分段（segmentation）时，却可以通过紧凑来减少碎片.

最后我们`假设`，分配程序所管理的是`连续的一块字节区域`。 在一些情况下，分配程序可以要求这块区域增长。
例如，一个用户级的内存分配库在空间快用完时，可以向内核申请增加堆空间（通过 sbrk 这样的系统调用），但是，简单起见，我们`假设`这块区域在`整个生命周期内大小固定`。

### 17.2 底层机制

在深入策略细节之前，我们先来了解大多数`分配程序采用的通用机制`.
首先，探讨空间分割与合并的基本知识。
其次，看看如何快速并相对轻松地追踪已分配的空间。
最后，讨论如何利用空闲区域的内部空间维护一个简单的列表，来追踪空闲和已分配的空间。

#### 分割与合并

空闲列表包含一组元素，记录了堆中的哪些空间还没有分配。假设有下面的 30 字节的堆：

- ![](assets/Pasted%20image%2020230329184604.png)

这个堆对应的空闲列表会有两个元素，一个描述第一个 10 字节的空闲区域（字节 0～9），
一个描述另一个空闲区域（字节 20～29)：

- ![](assets/Pasted%20image%2020230329184642.png)

可以看出，任何大于10 字节的分配请求都会失败（返回 NULL), 没有足够连续空间了.
如果申请小于 10 字节空间，会发生什么？

假设我们只申请一个字节的内存. 这种情况下, 分配程序会执行所谓的`分割(splitting)动作`: 
它找到一块可以满足请求的空闲空间，将其分割，第一块返回给用户，第二块留在空闲列表中.
在我们的例子中，假设这时遇到申请一个字节的请求，分配程序选择使用第二块空闲空间，对 malloc()的调用会返回 20（1 字节分配区域的地址），空闲列表会变成这样：

- ![](assets/Pasted%20image%2020230329185040.png)

除了第二个空闲区域的起始位置由20变成21, 长度由 10 变为 9 了, 其他没什么变化.
如果请求的空间大小小于某块空闲块，分配程序通常会进行`分割`。

许多分配程序中因此也有一种机制, 名为`合并(coalescing)`. 
还是前面的例子, 30个字节, 只有中间10个字节是used.

对于这个(小)堆, 如果应用程序调用free(10), `归还堆中间的空间`, 会发生什么?如果只是简单地将这块空闲空间加入空闲列表，不多想想，可能得到如下的结果：

- ![](assets/Pasted%20image%2020230329191929.png)
- 问题出现了：尽管整个堆现在完全空闲，但它似乎被分割成了 3 个 10 字节的区域。这时，如果用户请求 20 字节的空间，简单遍历空闲列表会找不到这样的空闲块，因此返回失败。

为了避免这个问题，分配程序会在`释放一块内存时合并可用空间`。
想法很简单：在归还一块空闲内存时，仔细查看`要归还的内存块的地址`以及`邻它的空闲空间块`。如果新归还的空间与一个原有空闲块相邻(或两个, 就像这个例子), 就将它们合并为一个较大的空闲块。通过合并，最后空闲列表应该像这样：

- ![](assets/Pasted%20image%2020230329192304.png)

实际上，这是堆的空闲列表最初的样子，在所有分配之前。通过合并，分配程序可以更好地确保大块的空闲空间能提供给应用程序。

#### 追踪已分配空间的大小

free(void \*ptr)接口没有块大小的参数, 因此它是假定，对于给定的指针，内存分配库可以很快`确定要释放空间的大小`, 从而将它放回空闲列表。

要完成这个任务，大多数分配程序都`会在头块(header)中保存一点额外的信息`，它在内存中，通常就`在返回的内存块`之前。

- ![](assets/Pasted%20image%2020230329192610.png)
- 看上面两个图, 设想用户调用了 malloc()并将结果保存在ptr 中：ptr = malloc(20). 实际上占用的不是20个字节, 而是20个字节加上头部信息. 是20多个字节. ptr保存可用堆内存的起始地址.
- 头块中, 保存了分配的size, 也可能包含一些额外的指针来加速空间释放，包含一个magic来提供完整性检查，以及其他信息。
- 假设一个简单的头块包含了分配空间的大小和一个幻数：

```c
typedef struct header_t {
    int size;
    int magic;
} header_t;
```
用户调用 free(ptr)时，库会通过简单的指针运算得到头块的位置：
```c
void free(void *ptr) {
	header_t *hptr = (void *)ptr - sizeof(header_t);
}
```
获得头块的指针后，库可以很容易地确定幻数是否符合预期的值, 作为正常性检查
`(assert(hptr->magic == 1234567))`, 并简单计算要释放的空间大小(即头块的大小加区域长度).
注意一个小但重要的细节：实际释放的是`头块大小`加上`分配给用户的空间的大小`.
因此，如果用户请求 N 字节的内存，库不是寻找大小为 N 的空闲块，而是寻找N 加上头块大小的空闲块。

#### 嵌入空闲列表

到目前为止, 我们这个简单的空闲列表还只是一个概念上的存在, 它就是一个列表, 描述了堆中的空闲内存块. 但如何在`空闲内存自己内部`建立这样一个列表呢? 

在更典型的列表中，如果要分配新节点，你会调用 malloc()来获取该节点所需的空间。
遗憾的是，在内存分配库内，你无法这么干！你需要在空闲空间本身中建立空闲空间列表。

假设我们需要管理一个 4096 字节的内存块（即堆是 4KB）。为了将它作为一个空闲空间列表来管理，首先要初始化这个列表。开始，列表中只有一个条目，记录了大小为4096的空间(减去头块的大小)。下面是该列表中一个节点描述：
```c
typedef struct node_t {
	int size;
	struct node_t *next;
} node_t;
```

现在来看一些代码，它们初始化堆，并将空闲列表的第一个元素放在该空间中。
假设堆构建在某块空闲空间上，这块空间通过系统调用 mmap()获得。这不是构建这种堆的唯一选择，但在这个例子中很合适。下面是代码：

```c
// mmap() returns a pointer to a chunk of free space
node_t *head = mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_ANON|MAP_PRIVATE, -1, 0);
head->size = 4096 - sizeof(node_t);
head->next = NULL;
```
执行这段代码之后，列表的状态是它只有一个条目，记录大小为 4088。
这是一个小堆，但对我们是一个很好的例子。
head 指针指向这块区域的起始地址, 假设是16K(尽管任何虚拟地址都可以).
如图 17.3 所示:

- ![](assets/Pasted%20image%2020230329205721.png)

现在假设有一个100字节的内存请求, 为了满足这个请求，库首先要找到一个足够大小的块。因为只有一个 4088 字节的块，所以选中这个块。
然后, 这个块被分割(split)为两块: 一块足够满足请求(以及头块), 一块是剩余的空闲块.
假设记录头块为8个字节(一个整数记录大小, 一个整数记录magic)，堆中的空间如图 17.4 所示。

- ![](assets/Pasted%20image%2020230329210432.png)
- 对于 100 字节的请求，库从原有的一个空闲块中分配了 108 字节，返回指向它的一个指针(在上图中用 ptr 表示), 并在其之前连续的 8 字节中记录头块信息, 供未来的free()函数使用. 同时将列表中的空闲节点缩小为 3980 字节.

再申请两个100字节, 再来看该堆，其中有 3 个已分配区域，每个 100（加上头块是 108）。这个堆如图 17.5所示.

- ![](assets/Pasted%20image%2020230329211557.png)
- 堆的前 324 字节已经分配,  只有一个节点（由 head 指向），但在 3 次分割后，现在大小只有 3764 字节. 如果用户程序通过 free()归还一些内存，会发生什么？

在这个例子中，应用程序调用 free(16500)，归还了中间的一块已分配空间(内存块的起始地址 16K加上前一块的 108，和这一块的头块的 8 字节.) 这个值在前图中用 sptr 指向.

库马上弄清楚了这块要释放空间的大小，并将空闲块加回空闲列表。假设我们将它插入到空闲列表的头位置，该空间如图 17.6 所示。

- ![](assets/Pasted%20image%2020230329212038.png)

现在的空闲列表包括一个小空闲块（100 字节，由列表的头指向）和一个大空闲块（3764字节）。
列表终于有不止一个元素了，空闲空间被分割成了两段.

最后一个例子：现在假设剩余的两块已分配的空间也被释放。没有合并，空闲列表将非常破碎，如图 17.7 所示。

- ![](assets/Pasted%20image%2020230329212228.png)
- 虽然整个内存空间是空闲的，但却被分成了小段，因此形成了碎片化的内存空间。解决方案很简单：遍历列表，合并（merge)相邻块。完成之后，堆又成了一个整体。

#### 让堆增长

讨论最后一个很多内存分配库中都有的机制.
具体来说, 如果堆中的内存空间耗尽, 应该怎么办? 最简单的方式就是返回失败. 在某些情况下这是唯一的选择，因此返回 NULL 也是一种体面的方式.

大多数传统的分配程序会从很小的堆开始, 当空间耗尽时, 再向操作系统申请更大的空间. 通常, 这意味着它们进行了某种系统调用(例如, 大多数UNIX系统中的 `sbrk`), 让堆增长. 操作系统在执行 sbrk 系统调用时, 会找到空闲的物理内存页, 将它们映射到请求进程的地址空间中去, 并返回新的堆的末尾地址. 这时, 就有了更大的堆, 请求就可以成功满足. 

内存管理底层机制小结:

>如果申请比空间块小的内存, 就执行分割; 把已分配的空间释放时, 分配程序会进行检查和合并空闲内存的操作. 
>free() 只需要传入指针, 是因为可以通过这个指针地址, 找到这块内存的header信息.
>嵌入空闲列表, 这个数据结构可以帮助管理空闲内存, 但会占用一点空间. 所以4096的一块内存, 实际可用, 会少一点.
>内存耗尽就返回NULL, 没问题. 大多数传统分配程序, 会从小的堆内存空间开始分配, 小内存的单位耗尽, 就向OS申请大单位的空间. 比如类UNIX的sbrk系统调用, 可用让堆增长.(会找到空闲的物理内存页，将它们映射到请求进程的地址空间中去，并返回新的堆的末尾地址).

### 17.3 基本策略

有了上面的底层机制, 看看管理空闲空间(可用内存)的一些基本策略.(算法思想)
这些方法大多基于简单的策略.

理想的分配程序可以同时保证快速和碎片最小化. 
事实上，由于分配及释放的请求序列是任意的(毕竟, 它们由用户程序决定), 任何特定的策略在某种不匹配的输入下都会变得非常差. 所以不说“最好”的策略, 而是介绍一些基本的选择, 并讨论它们的优缺点. 

#### 最优匹配

最优匹配(best fit)策略非常简单: 首先遍历整个空闲列表，找到和请求大小一样或更大的空闲块，然后返回这组候选者中最小的一块。这就是所谓的最优匹配(也可以称为最小匹配). 只需要遍历一次空闲列表, 就足以找到正确的块并返回.

最优匹配背后的想法很简单：选择最接它用户请求大小的块，从而尽量避免空间浪费。
这种方法有系统开销. 简单的实现在遍历查找正确的空闲块时, 要付出较高的性能代价.

#### 最差匹配

最差匹配(worst fit)方法与最优匹配相反, 它尝试找最大的空闲块, 分割并满足用户需求后, 将剩余的块(很大)加入空闲列表. 最差匹配尝试在空闲列表中保留较大的块, 而不是向最优匹配那样可能剩下很多难以利用的小块. 
最差匹配同样需要遍历整个空闲列表. 而且, 大多数研究表明它的表现非常差, 会导致过量的碎片, 同时还有很高的开销. 

#### 首次匹配

首次匹配(first fit)策略就是找到第一个足够大的块，将请求的空间返回给用户。同样，剩余的空闲空间留给后续请求。
首次匹配有速度优势(不必全部遍历一遍), 但有时会让空闲列表开头的部分有很多小块。所以, 分配程序如何管理空闲列表的顺序就变得很重要.
一种方式是基于地址排序（address-based ordering）。通过保持空闲块按内存地址有序，合并操作会很容易，从而减少了内存碎片。

#### 下次匹配

不同于首次匹配每次都从列表的开头查找, 下次匹配(next fit)算法多维护一个指针, 指向上一次查找结束的位置. 
其想法是将对空闲空间的查找操作扩散到整个列表中去, 避免对列表开头频繁的分割. 这种策略的性能与首次匹配很接近, 同样避免了遍历查找. 

#### 例子

下面是上述策略的一些例子。设想一个空闲列表包含 3 个元素，长度依次为 10、30、20.

- ![](assets/Pasted%20image%2020230329224807.png)

假设有一个 15 字节的内存请求。最优匹配会遍历整个空闲列表，发现 20 字节是最优匹配，因为它是满足请求的最小空闲块。结果空闲列表变为：

- ![](assets/Pasted%20image%2020230329224825.png)
    本例中发生的情况，在最优匹配中常常发生，现在留下了一个小空闲块。

最差匹配类似，但会选择最大的空闲块进行分割，在本例中是 30。结果空闲列表变为：

- ![](assets/Pasted%20image%2020230329224851.png)

在这个例子中，首次匹配会和最差匹配一样，也发现满足请求的第一个空闲块。
不同的是查找开销，最优匹配和最差匹配都需要遍历整个列表，而首次匹配只找到第一个满足需求的块即可，因此减少了查找开销。

### 17.4 其他方式

除了上述基本策略外, 人们还提出了许多技术和算法, 来改进内存分配. 这里我们列出一些来供你考虑(就是让你多一些思考, 不只局限于最优匹配). 

#### 分离空闲列表

segregated list, 基本想法很简单: 如果某个应用程序经常申请一种(或几种)大小的内存空间, 那就用一个独立的列表, 只管理这样大小的对象. 其他大小的请求都一给更通用的内存分配程序. 

这种方法的好处显而易见. 通过拿出一部分内存专门满足某种大小的请求, 碎片就不再是问题了。而且, 由于没有复杂的列表查找过程, 这种特定大小的内存分配和释放都很快. 

这种方式也为系统带来了新的问题. 例如, 应该拿出多少内存来专门为某种大小的请求服务, 而将剩余的用来满足一般请求? 超级工程师Jeff Bonwick为Solaris系统内核设计的`厚块分配程序(slab allocator)`,很优雅地处理了这个问题.

在内核启动时, 它为可能`频繁请求的内核对象`创建一些`对象缓存(object cache)`, 如锁和文件系统inode 等. 
这些对象`缓存每个分离了特定大小的空闲列表`, 因此能够很快地响应内存请求和释放.
如果某个缓存中的空闲空间快耗尽时，它就向通用内存分配程序申请一些内存厚块（slab）（总量是页大小和对象大小的公倍数）。
相反，如果给定厚块中对象的引用计数变为 0，通用的内存分配程序可以从专门的分配程序中回收这些空间，这通常发生在虚拟内存系统需要更多的空间的时候。

厚块分配程序比大多数分离空闲列表这得更多，它将列表中的空闲对象保持在预初始化的状态。Bonwick 指出，数据结构的初始化和销毁的开销很大.
通过将空闲对象保持在初始化状态，厚块分配程序避免了频繁的初始化和销毁，从而显著降低了开销.

#### 伙伴系统

因为合并对分配程序很关键，所以人们设计了一些方法，让合并变得简单，一个好例子就是`二分伙伴分配程序`（binary buddy allocator）.

在这种系统中，空闲空间首先从概念上被看成大小为 `2的N次方`大空间。
当有一个内存分配请求时，空闲空间被递归地一分为二，直到刚好可以满足请求的大小（再一分为二就无法满足）。这时，请求的块被返回给用户。在下面的例子中，一个 64KB 大小的空闲空间被切分，以便提供 7KB 的块：

- ![](assets/Pasted%20image%2020230330162927.png)
    在这个例子中, 最左边的 8KB 块被分配给用户(如上图中深灰色部分所示)。请注意, 这种分配策略只允许分配2的整数次幂大小的空闲块, 因此会有内部碎片(internal fragmen)的麻烦.

伙伴系统的经典之处在于`块被释放时`. 如果将这个 8KB 的块归还给空闲列表，分配程序会检查“伙伴”8KB 是否空闲. 如果是，就合二为一，变成 16KB 的块。然后再检查这个16KB块的伙伴是否空闲, 如果是, 就合并这两块.
这个递归合并过程继续上溯，直到合并整个内存区域，或者某一个块的伙伴还没有被释放。

伙伴系统好用的原因, 在于很容易确定某个块的伙伴. 
如何确定? 
上面例子中的各个块的地址, 会发现每对互为伙伴的块只有一位不同, 正是这一位决定了它们在整个伙伴树中的层次.

#### 其他想法

上面提到的众多方法都有一个重要的问题，缺乏可扩展性(scaling). 
具体来说，就是查找列表可能很慢。
因此，更先进的分配程序采用更复杂的数据结构来优化这个开销，牺牲简单性来换取性能。
例子包括平衡二叉树、伸展树和偏序树.
考虑到现代操作系统通常会有多核，同时会运行多线程的程序, 因此人们这了许多工作，提升分配程序在多核系统上的表现。

### 17.5 小结

在本章中，我们讨论了最基本的内存分配程序形式。
这样的分配程序存在于所有地方, 与你编写的每个 C 程序链接 , 也和管理其自身数据结构的内存的底层操作系统链接. 
与许多系统一样，在构建这样一个系统时需要这许多折中。对分配程序提供的确切工作负载了解得越多，就越能调整它以更好地处理这种工作负载。
在现代计算机系统中，构建一个适用于各种工作负载、快速、空间高效、可扩展的分配程序仍然是一个持续的挑战。

## Ch18 分页: 介绍

回归到内存管理的问题, 16章详细的讨论了分段这种方法, 是由进程运行时程序的几个段想到的. 这种方法天生的问题在于 外部碎片, 可用内存被分切成n块小段, 导致遇到大的需求时, 无法分配.

这章会介绍另一种方法, 将内存空间分割成固定长度的页片, 而不管你进程里的那几个段. 这就是分页.
分页不是将一个进程的地址空间分割成几个不同长度的逻辑段(即代码、堆、段)，而是分割成固定大小的单元，每个单元称为一页.
相应地，我们把物理内存看成是定长槽块的阵列, 叫作页帧(page frame). 每个这样的页帧包含一个虚拟内存页。我们的挑战是：

关键问题：如何通过页来实现虚拟内存
如何通过页来实现虚拟内存，从而避免分段的问题？基本技术是什么？如何让这些技术运行良好，并尽可能减少空间和时间开销？

### 18.1 一个简单例子

为了更好的理解分页机制，举一个简单例子来说明。

- ![](assets/Pasted%20image%2020230330171943.png)

图 18.1 展示了一个只有 64字节的小地址空间，有 4个16 字节的页(虚拟页 0、1、2、3).
(真实的地址空间肯定大得多，通常 32 位有 4GB 的地址空间，甚至有 64 位的.)

- ![](assets/Pasted%20image%2020230330172052.png)

物理内存，如图 18.2 所示，也由一组固定大小的槽块组成。在这个例子中，有 8 个页帧(由 128 字节物理内存构成，也是极小的)。
从图中可以看出，虚拟地址空间的页放在物理内存的不同位置。图中还显示，操作系统自己用了一些物理内存。

跟分段相比, 分页有很多优点: 
最大的改进就是灵活性:
通过完善的分页方法，操作系统能够高效地提供地址空间的抽象，不管进程如何使用地址空间。
例如，我们不会假定堆和栈的增长方向，以及它们如何使用。

另一个优点是分页提供的空闲空间管理的简单性。
例如，如果操作系统希望将 64 字节的小地址空间放到 8 页的物理地址空间中，它只要找到 4 个空闲页。也许操作系统保存了一个所有空闲页的空闲列表（free list），只需要从这个列表中拿出 4 个空闲页。在这个例子里，操作系统将地址空间的虚拟页 0 放在物理页帧 3，虚拟页 1 放在物理页帧 7，虚拟页 2放在物理页帧 5，虚拟页 3 放在物理页帧 2。页帧 1、4、6 目前是空闲的。

为了`记录`地址空间的每个虚拟页放在物理内存中的位置，操作系统通常为每个进程保存一个数据结构，称为`页表(page table)`.
页表的主要作用是为地址空间的每个虚拟页面`保存地址转换(address translation)`, 从而让我们知道每个页在物理内存中的位置. 
对于我们的简单示例（见图 18.2), 页表因此具有以下 4 个条目：(虚拟页 0→物理帧 3)、(VP 1→PF 7)、(VP 2→PF 5)和(VP 3→PF 2)。

注意: 这个页表是每一个进程的数据结构(我们讨论的大多数页表结构都是每个进程的数据结构, 有一个例外是倒排页表, inverted page table). 每个进程都有自己的虚拟地址空间, 所以必然有个页表.
如果在上面的示例中运行另一个进程，操作系统将不得不为它管理另一个页表，因为它的虚拟页显然映射到不同的物理页面(除了共享之外).

知道了上面的知识, 来看看怎么从虚拟地址转到物理地址.
设想拥有这个小地址空间（64 字节）的进程正在访问内存：`movl <virtual address>, %eax`.
注意从地址`<virtual address>`到寄存器 eax 的数据显式加载(因此忽略之前肯定会发生的指令获取).

为了转换(translate)该过程生成的虚拟地址, 我们必须首先将它分成两个组件：`虚拟页面号`(virtual page number，VPN) 和`页内的偏移量`(offset). 对于这个例子, 因为进程的虚拟地址空间是 64 字节, 我们的虚拟地址总共需要 6 位（2^6 = 64）。因此, 虚拟地址可以表示如下：

- ![](assets/Pasted%20image%2020230330184116.png)

在该图中，Va5 是虚拟地址的最高位，Va0 是最低位。因为我们知道页的大小（16 字节），所以可以进一步划分虚拟地址，如下所示：

- ![](assets/Pasted%20image%2020230330184132.png)
    页面大小为 16 字节, 位于 64 字节的地址空间. 因此我们需要能够选择 4 个页, 虚拟地址的高2位定位页号. 低4位就是页内的地址偏移量, 一个页就16个字节, 4位完全够用.

当进程生成虚拟地址时，操作系统和硬件必须协作，将它转换为有意义的物理地址。例如，让我们假设上面的加载是虚拟地址 21：`movl 21, %eax` .
将“21”变成二进制形式，是“010101”，因此我们可以检查这个虚拟地址，看看它是如何分解成虚拟页号（VPN）和偏移量的：

- ![](assets/Pasted%20image%2020230330185147.png)
    在虚拟页1号页, 页内地址为5的字节.
    通过虚拟页号, 去查页表, 找到虚拟页 1 所在的物理页面。
    在上面的页表中, 物理帧号(PFN)(也称为物理页号, physical page number或PPN)是 7(二进制111)
    通过用PFN替换VPN来转换此虚拟地址, 然后将载入发送给物理内存(见图 18.3).

- ![](assets/Pasted%20image%2020230330185626.png)
    然后去物理地址1110101的地方去取, 页内偏移量不用改, 因为相对位置在虚拟页跟物理页是一样的.

### 18.2 页表存在哪里

页表可以变得非常大, 比小段表或基址/界限对要大得多. 
例如,  32 位Linux的地址空间, 一页是4KB. 这个虚拟地址分成20位的VPN和12位的偏移量(12位就是4KB).
一个 20位的VPN意味着, 操作系统必须为每个进程管理2^20个`页表项`(一百万多一点). 每一个页表项用4个字节存, 那一个页表就是4MB. 这只是一个进程的哦, 如果有100个进程呢? 
400MB, 爆炸.  那要是64位的机器呢? 怎么办后面再讲.

页表太大了, 在寸土寸金的CPU内部, MMU没法存这么多数据. 只能`将每个进程的页表存储在内存中`. 

- ![](assets/Pasted%20image%2020230330194423.png)
    上图中, 页表就保存在了物理内存的0号页帧里了. 每次转换地址的时候, 就需要访存, 把虚拟地址转换成物理地址.

### 18.3 页表里究竟有什么

看看页表的组织.
`页表就是一种数据结构`, 用于将虚拟地址(虚拟页号)映射到物理地址(物理帧号). 
至于是什么数据结构, 最简单的就是线性的页表(linear page table), 比如顺序表. 就是一个数组.
操作系统通过`虚拟页号(VPN)`检索该数组, 并在该索引处`查找页表项`(PTE), 以便找到期望的物理帧号(PFN).
现在，我们假设采用这个简单的线性结构。在后面的章节中，我们将利用更高级的数据结构来帮助解决一些分页问题.

至于每个页表项的内容，在其中有许多不同的数据位，值得有所了解。`有效位(valid bit)`通常用于`指示特定地址转换是否有效`.
例如，当一个程序开始运行时，它的代码和堆在其地址空间的一端，栈在另一端。所有`未使用`的中间空间都将被标记为`无效`(invalid), 如果进程尝试访问无效内存，就会陷入内核，可能会导致该进程终止。
因此，`有效位对于支持稀疏地址空间至关重要`。
通过简单地将地址空间中所有`未使用的页面`标记为`无效`，就`不再需要为这些页面分配物理帧`，从而节省大量内存.

还有`保护位(protection bit)`，表明页是否可以读取、写入或执行。同样，以这些位不允许的方式访问页，会陷入内核..

`存在位(present bit)`表示该页是`在物理存储器还是在磁盘上`(即它是否已被换出，swapped out). 后面会讨论. 交换允许操作系统将很少使用的页面移到磁盘，从而释放物理内存.
`脏位(dirty bit)`也很常见，表明`页面被带入内存后是否被修改过.` 

`参考位(reference bit`, 也被称为`访问位, accessed bit)`有时用于追踪页是否被访问, 也用于确定哪些页很受欢迎，因此应该保留在内存中.这些知识在页面替换(page replacement)时非常重要，我们将在随后的章节中详细研究这一主题。

图 18.5 显示了来自 x86 架构的示例页表项。

- ![](assets/Pasted%20image%2020230330204430.png)

- 它包含一个`存在位(P)`, 
- 确定是否允许写入该页面的读/写位(R/W) 
- 确定用户模式进程是否可以访问该页面的用户/超级用户位(U/S), 
- 有几位(PWT、PCD、PAT 和 G)确定硬件缓存如何为这些页面工作，
- 一个访问位(A)和一个脏位(D),
- 最后是页帧号(PFN)本身.

### 18.4 分页: 也很慢

内存中的页表, 可能会太大了. 事实证明，它们也会让速度变慢。以简单的指令为例：`movl 21, %eax`.
同样，我们只看对地址21的显式引用, 而不关心取指令.
在这个例子中, 我们假定硬件为我们执行地址转换. 要获取所需数据, 系统必须首先将`虚拟地址`(21)转换为正确的`物理地址`(117). 因此，在从地址117 获取数据之前, `系统必须首先从进程的页表中提取适当的页表项`, 执行转换, 然后从物理内存中加载数据. 

为此, 硬件必须知道`当前正在运行的进程的页表的位置`.
现在假设一个`页表基址寄存器`(page-table base register)包含`页表的起始位置的物理地址`. 为了`找到想要的页表项的位置`, 硬件将执行以下功能: 

```c
VPN = (VirtualAddress & VPN_MASK) >> SHIFT  //获取虚拟页号
PTEAddr = PageTableBaseRegister + (VPN * sizeof(PTE)) //拿到页表项地址
```
在我们的例子中, VPN MASK 将被设置为 0x30 (十六进制 30, 或二进制 110000), 它从完整的虚拟地址中挑选出 VPN 位; SHIFT 设置为 4 (偏移量的位数), 这样我们就可以将VPN 位向右移动以形成正确的整数虚拟页号.
然后, 我们使用该值作为`页表基址寄存器`指向的PTE数组的索引. 

一旦知道了这个页表所在的物理地址, 硬件就可以从内存中获取 PTE, 提取 PFN, 并将它与来自虚拟地址的偏移量连接起来, 形成所需的物理地址.
```c
offset = VirtualAddress & OFFSET_MASK
PhysAddr = (PFN << SHIFT) | offset
1 // Extract the VPN from the virtual address
2 VPN = (VirtualAddress & VPN_MASK) >> SHIFT
3
4 // Form the address of the page-table entry (PTE)
5 PTEAddr = PTBR + (VPN * sizeof(PTE))
6
7 // Fetch the PTE
8 PTE = AccessMemory(PTEAddr)
9
10 // Check if process can access the page
11 if (PTE.Valid == False)
12 RaiseException(SEGMENTATION_FAULT)
13 else if (CanAccess(PTE.ProtectBits) == False)
14 RaiseException(PROTECTION_FAULT)
15 else
16 // Access is OK: form physical address and fetch it
17 offset = VirtualAddress & OFFSET_MASK
18 PhysAddr = (PTE.PFN << PFN_SHIFT) | offset
19 Register = AccessMemory(PhysAddr)
```

对于`每个内存引用`(无论是`取指令`还是`显式加载或存储`), `分页`都需要我们`执行一个额外的内存引用`, 以便首先从页表中获取地址转换. 工作量很大! 额外的内存引用开销很大, 在这种情况下, 可能会使进程减慢两倍或更多.

所以如果不好好设计硬件和软件, 页表会导致系统运行速度过慢, 并占用太多内存.
虽然分页看起来是内存虚拟化需求的一个很好的解决方案, 但这两个关键问题必须先解决. 

### 18.5 内存追踪

通过一个简单的内存访问示例, 来演示`使用分页时产生的所有内存访问`. 我们感兴趣的代码片段(用C写的, 名为 array.c)是这样的：
```c
int array[1000];
...
for (i = 0; i < 1000; i++)
	array[i] = 0;
```
>`现代操作系统`的内存管理子系统中`最重要`的数据结构之一就是`页表`.
>一般来说, 系统中`每个进程都有一个页表`.
>页表的确切结构要么由硬件(旧系统)确定, 要么由OS(现代系统)更灵活地管理.

首先，我们必须反汇编结果二进制文件(在Linux上使用objdump或在Mac上使用otool), 查看使用什么汇编指令来初始化循环中的数组. 以下是生成的汇编代码: 
```asm
0x1024 movl $0x0,(%edi,%eax,4)
0x1028 incl %eax
0x102c cmpl $0x03e8,%eax
0x1030 jne 0x1024
```

为了理解这个指令序列(在虚拟层和物理层)所访问的内存, 我们必须假设虚拟内存中代码片段和数组的位置, 以及页表的内容和位置.

对于这个例子, 我们假设一个大小为 64KB 的虚拟地址空间(不切实际地小). 我们还假定页帧大小为 1KB.

需要知道页表的内容, 以及它在物理内存中的位置. 假设有一个线性(基于数组)的页表, 它位于物理地址1KB(1024).

我们只需要关心为这个例子映射的几个虚拟页面.
由于页大小为 1KB, 虚拟地址1024驻留在虚拟地址空间的第二页(VPN = 1). 假设这个虚拟页映射到物理页帧4(VPN 1→PFN 4).

接下来是数组本身. 它的大小是4000字节(1000 整数), 我们假设它驻留在虚拟地址40000到44000(不包括最后一个字节). 它的虚拟页的十进制范围是 VPN = 39……VPN =42.
针对这个例子，让我们假设以下虚拟到物理的映射：
(VPN 39 → PFN 7), (VPN 40 → PFN 8), (VPN 41 → PFN 9), (VPN 42 → PFN 10).

现在准备好跟踪程序的内存引用了. 
运行时, 每次取指令将产生两次内存引用: 一次访问页表(页表在物理内存)以查找指令所在的物理页帧, 第二次是执行指令将数据取到CPU进行处理. 
另外, 在 mov 指令的形式中, 有一个显式的内存引用, 就又有一次页表访问(将数组虚拟地址转换为正确的物理地址), 然后操作数组.

- ![](assets/Pasted%20image%2020230330220651.png)

图 18.7 展示了前 5 次循环迭代的整个过程. 最下面的图显示了 y 轴上的指令内存引用(黑色虚拟地址和右边的实际物理地址). 

整个追踪的x轴显示循环的前5个迭代中内存访问. 每个循环有10次内存访问, 其中包括 4 次取指令, 一次显式更新内存, 以及5次页表访问.

### 18.6 小结

我们已经引入了`分页(paging)`的概念, 作为虚拟内存挑战的解决方案. 与以前的方法(如分段)相比, 分页有许多优点. 首先, 它不会导致外部碎片, 因为分页(按设计)将内存划分为固定大小的单元. 其次, 它非常灵活, 支持稀疏虚拟地址空间. 

然而, 实现分页支持而没有好好设计的话, 会导致较慢的机器(有许多额外的内存访问来访问页表)和内存浪费(内存被页表塞满而不是有用的应用程序数据). 

因此, 我们不得不努力想出一个分页系统, 它不仅可以工作, 而且工作得很好. 幸运的是, 接下来的两章将告诉我们如何去做. 

作业:
线性页表大小随着虚拟地址空间的增长而变大, 因为页帧大小不变, 虚拟地址空间越大, 页就越多. 页表项就越多, 页表也就越大.
线性页表大小随页帧的变大而变小, 因为虚拟地址空间不变, 页帧变大了, 页就变少了. 页表就变小了. 

## Ch19 分页: 快速地址转换(TLB)

我们知道了使用分页作为核心机制来实现虚拟内存, 可能会带来`较高的性能开销`. 
`因为页表存在物理内存上`, 所以取指, 间指, 执行周期, 都要`多次访存`, 进行地址转换.

关键问题：如何加速地址转换
如何才能加速虚拟地址转换，尽量避免额外的内存访问？需要什么样的硬件支持？操作系统该如何支持？

`地址转换旁路缓冲存储器(translation-lookaside buffer)`, 它就是频繁发生的虚拟到物理地址转换的`硬件缓存(cache)`. 更好的名称应该是`地址转换缓存`(address-translation cache).

对每次内存访问, `硬件先检查TLB`, 看看其中是否有期望的转换映射, 如果有, 就完成转换(很快), 不用访问页表(其中有全部的转换映射). TLB带来了巨大的性能提升, 实际上, 因此它使得虚拟内存成为可能. 
所以`TLB`是从`内存的页表`中, 取一部分`页表项`缓存在自己内部. (根据局部性原理).

### 19.1 TLB 的基本算法

图 19.1 展示了一个大体框架，说明硬件如何处理虚拟地址转换, 假定使用简单的线性页表(Linear page table, 即页表是一个数组) 和硬件管理的 TLB(hardware-managed TLB, 即硬件承担许多页表访问的责任, 下面会有更多解释). 

```c
1 VPN = (VirtualAddress & VPN_MASK) >> SHIFT
2 (Success, TlbEntry) = TLB_Lookup(VPN)
3 if (Success == True) // TLB Hit
4     if (CanAccess(TlbEntry.ProtectBits) == True)
5         Offset = VirtualAddress & OFFSET_MASK
6         PhysAddr = (TlbEntry.PFN << SHIFT) | Offset
7         AccessMemory(PhysAddr)
8     else
9         RaiseException(PROTECTION_FAULT)
10 else // TLB Miss
11     PTEAddr = PTBR + (VPN * sizeof(PTE))
12     PTE = AccessMemory(PTEAddr)
13     if (PTE.Valid == False)
14         RaiseException(SEGMENTATION_FAULT)
15     else if (CanAccess(PTE.ProtectBits) == False)
16         RaiseException(PROTECTION_FAULT)
17     else
18         TLB_Insert(VPN, PTE.PFN, PTE.ProtectBits)
19         RetryInstruction()
```
图 19.1 TLB 控制流算法

硬件算法的大体流程如下：首先从虚拟地址中提取页号（VPN）,然后检查 TLB 是否有该 VPN 的页表项。如果有, TLB 命中(TLB hit), 从TLB的页表项中取出页帧号(PFN), 与原来虚拟地址中的偏移量组合形成物理地址(PA), 并访问内存. 假定保护检查没有失败.

如果 CPU 没有在TLB中找到转换映射(TLB 未命中), 我们有一些工作要做. 在本例中, 硬件访问页表来寻找页表项(第 11～12 行), 并用该页表项更新TLB(第 18 行), 假设该虚拟地址有效, 而且我们有相关的访问权限(第 13、15 行). 
上述系列`操作开销较大`, 主要是因为访问页表需要`额外的内存引用`(第 12 行). 最后, 当 TLB 更新成功后, 系统会重新尝试该指令, 这时 TLB 中有了这个页表项, 内存引用得到很快处理。

TLB 和其他缓存相似，前提是在一般情况下，`页表项会在缓存中`（即命中）.
如果是这样，只增加了很少的开销，因为 TLB 处理器核心附近，设计的访问速度很快。
如果 `TLB未命中`，就会带来很大的分页开销。必须访问页表来查找页表项，导致`一次额外的内存引用`（或者更多，如果页表更复杂）。如果这经常发生，程序的运行就会显著变慢。
相对于大多数 CPU 指令，内存访问开销很大，TLB 未命中导致更多内存访问。因此，我们希望`尽可能避免 TLB 未命中`。

### 19.2 示例：访问数组

为了弄清楚 TLB 的操作，我们来看一个简单虚拟地址追踪，看看 `TLB 如何提高它的性能`.

在本例中，假设有一个由 10 个 4 字节整型数组成的数组，起始虚地址是 100。进一步假定，有一个 8 位的小虚地址空间，页大小为 16B。
我们可以把虚地址划分为 4 位的 VPN（有 16 个虚拟内存页）和 4 位的偏移量（每个页中有 16 个字节）。

- ![](assets/Pasted%20image%2020230331094136.png)

图 19.2 展示了该数组的布局, 在系统的16个16 字节的页上. 如你所见, 数组的第一项(`a[0]`)开始于(VPN=06, offset=04), 只有3个4字节整型数存放在该页. 数组在下一页(VPN=07)继续, 其中有接下来4项(`a[3] … a[6]`). 10 个元素的数组的最后 3 项(`a[7] … a[9]`)位于地址空间的下一页(VPN=08).

程序是个简单的循环, 访问数组中的每个元素, 类似下面的 C 程序:
```c
int sum = 0;
for (i = 0; i < 10; i++) {
	sum += a[i];
}
```
简单起见, 我们假装循环产生的内存访问只是针对数组(忽略变量i和sum, 以及指令本身).

当访问第一个数组元素(`a[0]`)时, CPU 会看到载入虚存地址100. 硬件从中提取VPN(VPN=06), 然后用它来检查TLB, 寻找有效的页表项. 假设这里是程序第一次访问该数组, 结果是TLB未命中.
未命中, 所以访存查找页表, 找到对应的项, 然后把该页表项的加载到TLB缓存. 
所以访问`a[0] a[2]`都会命中. 而访问`a[3]`时, 因为是第一次访问该页, 所以TLB未命中, 需要访存页表, 并缓存页表项. 然后在VPN=07的这一页中的其他数据都能命中了.
以此类推.

我们来总结一下这 10 次数组访问操作中 TLB 的行为表现：只有三次初次访问某页的时候未命中, 其他都是命中的. 命中的次数除以总的访问次数，得到TLB命中率(hit rate)为70%. 不是特别高(假设第一次访问`a[0]`, 就把其余两个页也加载到TLB, 命中率就是90%.

也要`注意页大小`对本例结果的影响. 如果页大小变大一倍(32字节, 而不是16), 数组访问遇到的`未命中更少`. 典型页的大小一般为4KB, 这种情况下, 密集的, 基于数组的访问会实现极好的TLB 性能, 每页的访问`只会遇到一次`未命中.

关于TLB性能还有最后一点: 如果在这次循环后不久, 该程序再次访问该数组, 我们会看到更好的结果, 假设TLB足够大, 能缓存所需的转换映射: 全部命中. 在这种情况下, 由于`时间局部性`(temporal locality), 即在短时间内对内存项再次引用, 所以TLB的命中率会很高. 类似其他缓存, TLB的成功依赖于`空间和时间局部性`. 如果某个程序表现出这样的`局部性`(许多程序是这样), TLB 的命中率可能很高.

>提示：尽可能利用缓存
>`缓存`是计算机系统中最基本的性能改进技术之一, 一次又一次地用于让“常见的情况更快”.
>硬件缓存背后的思想是利用`指令和数据`引用的`局部性`(locality).
>`时间局部性`是指，最近访问过的指令或数据项可能很快会再次访问。
>`空间局部性`是指，当程序访问内存地址 x 时，可能很快会访问邻近 x 的内存.
>当然, 这些性质取决于程序的特点, 并不是绝对的定律, 而更像是一种经验法则.

如果想要`快速地缓存`, 它就`必须小`, 因为`光速`和`其他物理限制`会起作用. `大的缓存注定慢`, 因此无法实现目的. 所以, 我们`只能用小而快的缓存`. 

### 19.3 谁来处理TLB未命中

可能有两个答案: 硬件或软件(OS). 
以前搞复杂指令集的人, 不信任搞OS的. 因此, 硬件全权处理TLB未命中. 
为了做到这一点, 硬件必须知道页表在内存中的确切位置(通过`页表基址寄存器`, page-table base register), 以及页表的确切格式. 发生未命中时, 硬件会“遍历”页表, 找到正确的页表项, 取出想要的转换映射, 用它更新 TLB, 并重试该指令.
这种“旧”体系结构有硬件管理的 TLB，一个例子是 `x86 架构`，它采用`固定的多级页表`(multi-level page table). 当前页表由`CR3寄存器`指出.

现代精简指令集(如MIPS等), 有所谓的`软件管理TLB`(software-managed TLB). 发生TLB未命中时, 硬件系统会抛出一个异常. 就会暂停当前的指令流, 将特权级提升至内核模式，跳转至陷阱处理程序(trap handler). 这个处理程序是内核的代码, 用于处理TLB未命中.
代码执行时, 会去物理内存查找页表中的页表项, 然后用特别的“特权”指令更新TLB, 并从陷阱返回. 此时, 硬件会重试该指令(导致TLB命中的那个).

```c
1 VPN = (VirtualAddress & VPN_MASK) >> SHIFT
2 (Success, TlbEntry) = TLB_Lookup(VPN)
3 if (Success == True) // TLB Hit
4     if (CanAccess(TlbEntry.ProtectBits) == True)
5         Offset = VirtualAddress & OFFSET_MASK
6         PhysAddr = (TlbEntry.PFN << SHIFT) | Offset
7         Register = AccessMemory(PhysAddr)
8     else
9         RaiseException(PROTECTION_FAULT)
10 else // TLB Miss
11 RaiseException(TLB_MISS)
```

接下来讨论几个重要的细节. 
首先，这里的`从陷阱返回指令`稍稍不同于之前提到的`服务于系统调用的从陷阱返回`.
在后一种情况下, 从`陷阱返回`应该继续执行陷入操作系统之后那条指令, 就像从函数调用返回后, 会继续执行此次调用之后的语句. 
在前一种情况下, 在从`TLB未命中的陷阱`返回后, 硬件必须从`导致陷阱`的指令`继续执行`. 这次重试因此导致该指令再次执行, 但这次会命中TLB. 
因此, 根据陷阱或异常的原因, `系统`在`陷入内核时`必须`保存不同的程序计数器`, 以便将来能够正确地继续执行.

第二, 在运行 TLB 未命中处理代码时，操作系统需要格外小心`避免引起 TLB 未命中的无限递归`.
有很多解决方案:

例如, 可以把TLB未命中`陷阱处理程序`直接放到`物理内存`中(它们`没有映射过`(unmapped), 不用经过地址转换). 或者在TLB中保留一些项, 记录`永久有效的页表项`, 并将其中一些`永久地址转换槽块留给处理代码本身`, 这些`被监听的`(wired)地址转换总是会命中TLB. 

软件管理的方法, 主要`优势`是`灵活性`: 操作系统可以用任意数据结构来实现页表, 不需要改变硬件. 另一个优势是`简单性`. 从TLB控制流中可以看出硬件不需要对未命中做太多工作, 它抛出异常, 内核的未命中处理程序会负责剩下的工作. 

### 19.4 TLB 的内容

看一下硬件TLB中的内容. 
典型的TLB有`32项`, `64项`或`128项`, 并且是`全相联的`(fully associative).
基本上, 这就意味着一条地址映射可能存在TLB中的任意位置, `硬件会并行地`查找TLB, 找到期望的页表项. 一条TLB项内容可能像下面这样: 
`虚拟页号 ｜ 物理页号 ｜ 其他位`

注意, 虚拟页号和物理页号同时存在于TLB中, 因为一条页表项可能出现在任意位置(用硬件的术语, TLB被称为全相联的(fully-associative)缓存). 硬件`并行`地查找这些项, 看看是否有匹配.

>补充: `TLB`的有效位!=`页表`的有效位
>常见的错误是混淆 `TLB的有效位`和`页表的有效位`. 
>在页表中, 如果一个`页表项(PTE)`被标记为`无效`, 就意味着`该页并没有被进程申请使用`, 正常运行的程序不应该访问该地址. 当程序试图访问这样的页时, 就会陷入操作系统, 操作系统会杀掉该进程.
>TLB的有效位不同, 只是`指出TLB项是不是有效的地址映射`(页表项).
>例如, 系统启动时, 所有的TLB项通常被初始化为无效状态, 因为还没有地址转换映射被缓存在这里. 一旦启用虚拟内存, 当程序开始运行, 访问自己的虚拟地址, TLB 就会慢慢地被填满, 因此有效的项很快会充满TLB. 

其他位: 例如，`TLB `通常有一个`有效(valid)位`，用来标识该项是不是有效地转换映射。通常还有一些`保护(protection)位`，用来标识该页是否有`访问权限`。例如，代码页被标识为可读和可执行，而堆的页被标识为可读和可写。还有其他一些位，包括`地址空间标识符`（address-space identifier）、`脏位`（dirty bit）等。下面会介绍更多信息.

### 19.5 上下文切换时对TLB的处理

有了 TLB，在`进程间切换`时（因此有地址空间切换）, 会面临一些新问题.
TLB中包含的虚拟到物理的地址映射只对`当前进程`有效, 对其他进程是没有意义的.
所以在发生进程切换时, 硬件或操作系统(或二者)必须注意确保即将运行的进程`不要误读了`之前进程的地址映射.

看个例子,  进程p1假设TLB已经缓存了p1的页表项. 假设p1的10号虚拟页号 对应的物理页号是100.  假设系统里还有一个进程p2, 后面就会切换到进程p2. p2的10号虚拟页对应的物理页号是170. 
那么等OS完成上下文切换, 假设TLB没做什么, 那转换地址的时候, 就没办法确定找哪个页表项才好.
所以我们还需要做一些工作，`让 TLB 正确而高效地支持跨多进程的虚拟化`。因此, 关键问题是:

关键问题：进程切换时如何管理TLB的内容
如果发生进程间上下文切换, 上一个进程在 TLB 中的地址映射对于即将运行的进程是无意义的.
硬件或操作系统应该做些什么来解决这个问题呢? 

这个问题有一些可能的解决方案:
一种方法是在上下文切换时，简单地`清空(flush)TLB`, 这样在新进程运行前 TLB 就变成了空的。
如果是软件管理 TLB 的系统，可以在发生上下文切换时，通过一条显式(特权)指令来完成。如果是硬件管理 TLB，则可以在页表基址寄存器内容发生变化时清空 TLB(注意, 在上下文切换时, `内核必须改变页表基址寄存器`(PTBR)的值.)
清空操作都是把全部`有效位(valid)置为 0`, 本质上清空了TLB.

上下文切换的时候清空TLB, 这是一个可行的解决方案, 进程不会再读到错误的页表项. 但是，`有一定开销`: 每次进程运行, 当它访问数据和代码页时, 都会触发TLB未命中. 如果操作系统频`繁地`切换进程, 这种`开销会很高`.

`为了减少这种开销`, 一些系统增加了硬件支持, 实现`跨上下文切换的TLB共享`. 比如有的系统在 TLB中添加了一个`地址空间标识符`(Address Space Identifier, ASID).可以把ASID 看作是`进程标识符`(Process Identifier, PID), 但通常比 PID 位数少（PID一般32位, ASID一般是8位).

如果仍以上面的 TLB 为例，加上 ASID，很清楚不同进程可以`共享 TLB `了: 只要 ASID字段来区分原来无法区分的地址映射。表 19.2 展示了添加ASID字段后的TLB.

- ![](assets/Pasted%20image%2020230331171403.png)

因此, 有了地址空间标识符, TLB 可以`同时缓存不同进程的地址空间映射`, 没有任何冲突。
当然, 硬件也需要知道当前是哪个进程正在运行, 以便进行地址转换, 因此内核在`上下文切换时`, 必须将某个`特权寄存器`设置为`当前进程的ASID`.

补充一下, 你可能想到了另一种情况, TLB 中某两项非常相似. 在表19.3中, 属于两个不同进程的两项, 将两个不同的VPN指向了相同的物理页. 

- ![](assets/Pasted%20image%2020230331173137.png)

如果两个进程`共享同一物理页`(例如代码段的页), 就可能出现这种情况. 在上面的例子中, 进程 P1 和进程P2共享101号物理页, 但是P1将自己的10号虚拟页映射到该物理, 而P2将自己的50号虚拟页映射到该物理页. `共享代码页(以二进制或共享库的方式)`是有用的, 因为它减少了物理页的使用, 从而减少了内存开销. (也可能是多进程共享的内存页).

### 19.6 TLB 替换策略

TLB和其他缓存一样, 还有一个问题要考虑, 即`缓存替换`(cache replacement). 具体来说, 向TLB中插入新项时, 会替换(replace)一个旧项, 这样问题就来了: 应该替换那一个?

关键问题：如何设计 TLB 替换策略
在向TLB添加新项时, 应该替换哪个旧项? 目标当然是`减小TLB未命中率`(或`提高命中率`), 从而`改进性能.` 

在讨论页换出到磁盘的问题时, 我们将详细研究这样的策略. 这里我们先简单指出几个典型的策略. 

一种常见的策略是`替换最近最少使用`(least-recently-used, LRU)的项. LRU尝试利用内存引用流中的`局部性`，假定最近没有用过的项，可能是好的换出候选项.
另一种典型策略就是`随机`(random)策略, 即随机选择一项换出去. 这种策略很简单, 并且可以`避免一种极端情况`.
例如, 一个程序循环访问n+1个页, 但TLB大小只能存放n个页。
这时之前看似“合理”的LRU策略就会表现得不可理喻, 因为每次访问内存都会触发TLB未命中, 而随机策略在这种情况下就好很多.

### 19.7 实际系统的TLB表项

简单看一下真实的 TLB. CPU是`MIPS架构`, 精简指令集, 所以是采用`软件管理 TLB`.
图 19.4 展示了稍微简化的 MIPS TLB 项。

- ![](assets/Pasted%20image%2020230331180559.png)

32 位的地址空间, 页大小为4KB. 
在典型的虚拟地址中, 预期会看到`20位的VPN`和`12位的偏移量`. 但是, 你可以在TLB中看到, 只有 `19位的VPN`.
事实上, `用户地址只占地址空间的一半`(剩下的留给内核), 所以只需要19 位的VPN. `VPN转换成最大24位的物理帧号`(PFN), 因此可以支持最多有64GB 物理内存(`2^24`个4KB 内存页)的系统。

`MIPS TLB` 还有一些有趣的标识位。

比如`全局位`（Global，G），用来指示`这个页是不是所有进程全局共享`的。因此，如果全局位置为 1，就会忽略 ASID。我们也看到了 8 位的 ASID，操作系统用它来区分进程空间.

这里有一个问题: 如果正在运行的进程数超过256(2^8)个怎么办? (往往会有进程数限制.)

最后，我们看到 3 个`一致性位`(Coherence，C), 决定`硬件如何缓存该页`(其中一位超出了本书的范围);
`脏位(dirty)`, 表示`该页是否被写入新数据`(后面会介绍用法);
`有效位(valid)`, 告诉硬件`该项的地址映射是否有效`.
还有没在图19.4中展示的`页掩码`(page mask)字段, 用来`支持不同的页大小`. 
最后, 64位中有一些未使用(图19.4中灰色部分).

`MIPS的TLB通常有32项或64项`(`每一项8个字节`)，大多数提供给用户进程使用，也有一小部分留给操作系统使用。`操作系统`可以`设置一个被监听的寄存器`，告诉硬件需要`为自己预留多少TLB槽`。这些保留的页表项，被操作系统用于保存关键时候它要使用的代码和数据，在这些时候，TLB 未命中可能会导致问题（例如，在 TLB 未命中处理程序中）。

由于MIPS的TLB是软件管理的, 所以系统需要提供一些`更新TLB的指令`. MIPS提供了4个这样的指令: 

- `TLBP`, 用来查找指定的转换映射是否在TLB中; 
- `TLBR`，用来将 TLB中的内容读取到指定寄存器中；
- `TLBWI`，用来替换指定的 TLB 项；
- `TLBWR`，用来随机替换一个 TLB 项。操作系统可以用这些指令管理 TLB 的内容。
- 当然这些指令是特权指令，这很关键。

补充: `Culler 定律`:
RAM暗示你访问 RAM 的任意部分都一样快, 但因为TLB这样的硬件/操作系统功能, 访问某些内存页的开销较大, 尤其是没有被TLB缓存的页.
有时候随机访问地址空间，尤其是TLB没有缓存的页，可能导致严重的性能损失. David Culler 过去常常指出 TLB 是许多性能问题的源头, 所以以他来命名这个定律：Culler定律(Culler’s Law).

### 19.8 小结

介绍了硬件如何让地址转换更快的方法. 
通过增加一个小的、芯片内的 `TLB 作为地址转换的缓存`，大多数内存引用就不用访问内存中的页表了。因此，在大多数情况下，程序的性能就像内存没有虚拟化一样，这是操作系统的杰出成就，当然对现代操作系统中的分页非常必要。

但是，TLB 也不能满足所有的程序需求。具体来说，如果一个程序短时间内访问的页数超过了 TLB中的页表项数，就会产生大量的 TLB 未命中，运行速度就会变慢。
这种现象被称为`超出 TLB 覆盖范围`（TLB coverage），这对某些程序可能是相当严重的问题。
解决这个问题的一种方案是`支持更大的页`，把关键数据结构放在程序地址空间的某些区域，这些区域被映射到更大的页，`使 TLB 的有效覆盖率增加`。
对更大页的支持通常被`数据库管理系统(DBMS)`这样的程序利用, 它们的数据结构比较大, 而且是随机访问.

另一个TLB问题值得一提: 访问TLB很容易成为`CPU流水线的瓶颈`, 尤其是有所谓的`物理地址索引缓存`(physically-indexed cache). 有了这种缓存, `地址转换`必须发生在`访问该缓存之前`, 这会让操作变慢.
为了解决这个潜在的问题, 人们研究了各种巧妙的方法, `用虚拟地址直接访问缓存`, 从而在缓存命中时避免昂贵的地址转换步骤. 像这种`虚拟地址索引缓存`(virtually-indexed cache)解决了一些性能问题, 但也为硬件设计带来了新问题.

作业:
gettimeofday() 是微秒级, 可以多次访存, 达到微妙, 然后取平均.
后续碰到问题是tv_usec循环计数太快，如果尝试次数太多，容易产生tv_usec结束时间小于开始时间的现象，但是改用tv_sec,加大循环次数就很容易时间过长，或者由于精度不够无法计算出正常的结果。

## Ch20 分页: 较小的表

现在来解决分页引入的第二个问题: 页表太大, 因此消耗的内存太多.
假设一个32位地址空间(2^32字节), 4KB(2^12字节)的页和一个4字节的页表项. 一个地址空间中大约有一百万个虚拟页面(`2^32/2^12`). 乘以页表项的大小, 你会发现`一个进程的页表大小为 4MB`. 有`一百个活动进程`, 就要`400MB`.

关键问题: 如何让页表更小? 
简单的基于数组的页表(通常称为`线性页表`)太大, 在典型系统上占用太多内存. 如何让页表更小? 
关键的思路是什么? 由于这些新的数据结构, 会出现什么效率影响?

### 20.1 简单的解决方案：更大的页

页表的大小 = 存一个页表项的size x 页表项的个数.
页表项的个数 = 地址空间的size ÷ 一页的size.

所以,  假设把一页的size从4KB变成16KB, 还是32位虚拟空间, 就会有18位的VPN, 加14位的页内偏移量. 一个页表项还用4个字节存储, 现在一个页表中, 就有2^18个页表项, 所以一个页表, 大小就是1MB. 

这种方法的主要问题在于, 大内存页会导致每页内的浪费, 这被称为`内部碎片`(internal fragmentation)问题(因为浪费在分配单元内部). 因此, 结果是应用程序会分配页, 但只用每页的一小部分，而内存很快就会充满这些过大的页。因此，大多数系统在常见的情况下使用相对较小的页大小：4KB (如x86) 或 8KB (如SPARCv9). 

### 20.2 混合方法: 分页和分段

- ![](assets/Pasted%20image%2020230401174705.png)
- 假设我们有一个地址空间, 其中堆和栈的使用部分很小. 例如, 我们使用一个16KB 的小地址空间和 1KB 的页(见图 20.1). 该地址空间的页表如表 20.1 所示. 从图20.1中可以看到, 大部分页表都没有使用, 充满了无效的(invalid)项. 真是太浪费了！这是一个微小的16KB地址空间. 想象一下32位地址空间的页表和所有潜在的浪费空间!

`段页式`方法不是为进程的整个地址空间提供单个页表，而是`每个逻辑段一个页表`。在这个例子中，我们可能有 3 个页表，地址空间的代码、堆和栈部分各有一个.

分段中, 有一个`基址(base)寄存器`, 告诉我们每个段在物理内存中的位置, 还有一个界限(bound)或限制(limit)寄存器, 告诉我们该段的大小. 
在段页式方案中，我们仍然在 MMU 中拥有这些结构. 在这里, 我们使用`基址`不是指向段本身, 而是`保存该段的页表`的`物理地址`. `界限寄存器`用于指示`页表的结尾`(即它有多少有效页).

举个例子:
假设 32 位虚拟地址空间, 4KB 一页，并且地址空间分为4个段. 在这个例子中, 我们只使用3个段: 一个用于代码，另一个用于堆，还有一个用于栈。

确定地址引用哪个段，我们会用地址空间的前两位。假设 00 是未使用的段，01 是代码段，10 是堆段，11 是栈段。因此，虚拟地址如下所示：

- ![](assets/Pasted%20image%2020230401182700.png)
- 在硬件中，假设有3对基本/界限寄存器，代码、堆和栈各一对。当进程正在运行时，每个段的`基址寄存器`都包含`该段的线性页表的物理地址`。系统中的每个进程现在都有3个与其关联的页表. 在上下文切换时，必须更改这些寄存器，以反映新运行进程的页表的位置.

在 TLB 未命中时（假设硬件管理的 TLB，即硬件负责处理 TLB 未命中），硬件使用分段位（SN）来确定要用哪个基址和界限对。然后硬件将其中的物理地址与 VPN 结合起来，形成页表项（PTE）的地址：
```c
SN = (VirtualAddress & SEG_MASK) >> SN_SHIFT
VPN = (VirtualAddress & VPN_MASK) >> VPN_SHIFT
AddressOfPTE = Base[SN] + (VPN * sizeof(PTE))
```

段页式方案的关键区别在于，每个分段都有界限寄存器，每个界限寄存器保存了段中最大有效页的值。
例如，如果代码段使用它的前 3 个页（0、1 和 2），则代码段页表将只有 3个项分配给它，并且界限寄存器将被设置为 3。内存访问超出段的末尾将产生一个异常，并可能导致进程终止。以这种方式，与线性页表相比，杂合方法实现了显著的内存节省。栈和堆之间未分配的页不再占用页表中的空间（仅将其标记为无效）。

如果有一个大而稀疏的堆，仍然可能`导致大量的页表浪费`。其次，这种杂合`导致外部碎片再次出现`。尽管大部分内存是以页面大小单位管理的，但页表现在可以是任意大小（是 PTE 的倍数）。因此，在内存中为它们寻找自由空间更为复杂。
出于这些原因，`段页式其实不太常用`.

### 20.3 多级页表

如何`去掉页表中的所有无效区域`，而不是将它们全部保留在内存中?
我们将这种方法称为`多级页表`(multi-level page table), 因为它`将线性页表变成了类似树的东西`. 这种方法非常有效, 许多现代系统都用它.

多级页表的基本思想很简单:

- 首先, 将`页表(的size)分成物理页帧大小的若干单元`. 
- 然后, 如果`整页的页表项`(PTE)全都无效, 就`完全不分配该页的页表`。
    - 为了`追踪页表的页是否有效`(以及如果有效, 它在`内存中的位置`), 使用了名为`页目录`(page directory)的新结构. 
    - 页目录因此可以告诉你页表的页在哪里, 或者页表的整个页不包含有效页.

- ![](assets/Pasted%20image%2020230401193132.png)

上图左边是一级页表, 就算是虚拟空间的大部分区域都无效, 也的必须存储页表. 
右边是`多级页表`, `页目录`仅仅`将页表的两页`标记为`有效`; 因此, `页表的这两页就驻留在内存中`.

所以多级页表的工作方式是: 它只是让线性页表的一部分消失(释放这些帧用于其他用途), 并用页目录来记录页表的哪些页被分配.

在一个简单的两级页表中, 页目录由多个`页目录项`(PageDirectory Entries, PDE)组成.
`页目录的一项代表的是存放在某个物理内存页帧的页表. `
PDE(至少)拥有`有效位(valid bit)`和`物理页帧号`(page frame number, PFN), 类似于 PTE.
如果`PDE项是有效`的, 则意味着该项指向的`页表内`(通过 PFN)中至少有一个`页表项`是有效的, 即在该 PDE 所指向的页中, 至少一个 PTE, 其有效位被设置为 1. 如果 PDE 项无效(即等于零), 则 PDE的其余部分没有定义.

多级页表的优势:
首先, 最明显的是, 多级页表分配的页表空间, 与你正在使用的地址空间内存量成比例. 因此它通常很紧凑, 并且支持稀疏的地址空间.
其次, 如果仔细构建, 页表的每个部分都可以整齐地放入一个物理页中, 从而更容易管理内存. 
操作系统可以在需要分配或增长页表时简单地获取下一个空闲页. 
将它与一个简单的(非分页)线性页表相比, 后者仅是按VPN索引的PTE数组. 用这样的结构，整个线性页表必须连续驻留在物理内存中. 对于一个大的页表(比如 4MB), 找到如此大量的, 未使用的连续空闲物理内存, 可能是一个相当大的挑战. 
有了多级结构, 我们增加了一个`间接层`(level of indirection), 使用了`页目录`, 它指向页表的各个部分. 这种间接方式, 让我们能够将页表页放在物理内存的任何地方. 

`多级页表也是有成本的`. 在TLB未命中时, 需要从内存`加载两次`, 才能从页表中获取正确的地址转换信息(一次用于页目录, 另一次用于 PTE 本身), 而用线性页表只需要一次加载.
因此，多级表是一个`时间—空间折中`（time-space trade-off）的小例子.
空间上得到了缓解, 但时间上有些损失.

另一个明显的缺点是`复杂性`. 
无论是硬件还是操作系统来处理页表查找(在 TLB 未命中时), 这样做无疑都比简单的线性页表查找更复杂。通常我们愿意增加复杂性以提高性能或降低管理费用。在多级表的情况下，为了节省宝贵的内存，我们使页表查找更加复杂。

#### 详细的多级示例

为了更好地理解多级页表背后的想法, 我们来看一个例子.
假设一个大小为16KB(2^14)的小地址空间, 页大小是64(2^6)字节. 
虚拟地址空间是14位的, VPN是8位, 页内偏移地址是6位.
即使只有一小部分虚拟地址空间正在使用, 线性页表也会有2^8(256)个页表项.
图 20.3 展示了这种地址空间的一个例子.

- ![](assets/Pasted%20image%2020230401220725.png)
    - 在这个例子中, 虚拟页0和1用于代码, 虚拟页4和5用于堆, 虚拟页254和255用于栈. 地址空间的其余页未被使用.

要为这个地址空间构建一个`两级页表`, 我们`从完整的线性页表开始, 将它分解成页大小的单元.`
完整页表(此例中)有`256个项`; 假设`每个页表项用4个字节存`. 那么`一个页表(每个进程)大小就是1KB`.
但是`物理页帧` `一页是64字节`. 所以一个页表(1KB)就可以分为16个64字节, 即需要`16个物理页帧`. 或者`一个物理页帧`, 可以存放`16个页表项`. (或者64/4也可以得到一个物理页帧可以放多少个页表项).

我们现在需要了解: 如何获取VPN, 并用它来`首先索引到页目录`中, 然后`再索引到页表的页`中. 请记住, 每个都是一组项. 因此, 我们需要弄清楚, 如何为每个 VPN 构建索引.

我们首先`索引到页目录`. 

这个例子中的页表很小: 256个项, 分布在16个物理页帧上. `页目录`需要为`存放页表项的每个物理页`提供一个`页目录项`. 因此, 它有`16个项`. 结果, 我们需要`4位VPN来索引目录`. 我们`使用VPN的前4位`, 如下所示:

- ![](assets/Pasted%20image%2020230401223609.png)
- 4位页目录, 16个物理页帧, 剩下的4位VPN, 1个物理页帧里16个页表项. 比如 0000这个页目录, 表示物理页帧0, 里面有16个页表项.
- 一旦从 VPN 中提取了`页目录索引`(简称 `PDIndex`), 我们就可以通过简单的计算来找到`页目录项(PDE)`的地址: `PDEAddr = PageDirBase + (PDIndex * sizeof(PDE))`
- 确定了页目录, 现在我们来看它, 在地址转换上取得进一步进展.

如果`页目录项标记为无效`, 则我们知道访问无效, 从而`引发异常`. 但是, 如果PDE有效, 我们还有更多工作要做. 

具体来说, 我们现在必须从`页目录项`指向的存放页表项的`物理页中`获取`页表项`(PTE). 要找到这个 PTE, 我们必须使用 VPN 的剩余位索引到页表的部分:

- ![](assets/Pasted%20image%2020230402120529.png)
- 这个`页表索引`(Page-Table Index, `PTIndex`)可以用来`索引页表本身`, 给出 PTE 的地址: `PTEAddr = (PDE.PFN << SHIFT) + (PTIndex * sizeof(PTE))`
- 注意，从`页目录项`获得的`页帧号`(PFN)必须左移4位(因为这个值应该就是在高4位), 然后`再与页表索引组合`, 才能形成 PTE 的地址。

为了确定这一切是否合理，我们现在代入一个包含一些实际值的多级页表，并转换一个虚拟地址。让我们从这个例子的`页目录`开始（见表 20.2 的左侧）。

- ![](assets/Pasted%20image%2020230402133629.png)

    - 可以看到每个`页目录项`(PDE)都描述了有关地址空间页表的一些内容。在这个例子中, 地址空间里有`两个有效区域`(在开始和结束处)，以及一些无效的映射.
    - 在物理页100 (存页表的第 0 页的物理帧号)中, 我们有 1 页, 包含 16 个页表项, 记录了地址空间中的前 16 个 VPN. 请参见表 20.2(中间部分)了解这部分页表的内容.

    - 页表的这一页包含前16个VPN的映射. 在我们的例子中,` VPN0和1`两页是有效的(代码段), `4和5(堆)`两页也是. 因此, 该表有`每个页的映射信息`. 其余项标记为无效. 

    - 页表的另一个有效页在PFN101中. 该页包含地址空间的最后16个VPN的映射. 具体见表 20.2（右侧）.

    - 在这个例子中, 我们不是为一个线性页表分配完整的16页, 而是`分配3页`：一个用于`页目录`，两个用于`页表`的具有有效映射的块. 大型(32 位或64 位)地址空间的节省显然要大得多.

    - 最后, 让我们用这些信息来进行地址转换. 这里是一个地址, 指向 VPN 254 的第 0 个字节: 0x3F80. 二进制: 11 1111 1000 0000.
    - 高 4 位来索引页目录: 1111 就是15, `页目录第十五个项`, 即PFN 101. 在物理地址101的页里. 接下来四位 1110 找到具体的`页表项`, `第14个页表项`, 对应`物理页号是55`.(0x37). 
    - 页内偏移地址 offset = 000000. 物理地址就是: `PhysAddr =(PTE.PFN << SHIFT) + offset = 00 1101 1100 0000 = 0x0DC0.`

然而, 有时两个页级别是不够的！

#### 超过两级页表

假设我们有一个 30 位的虚拟地址空间和一个小的(512 字节)页。因此我们的虚拟地址有一个 21 位的虚拟页号和一个 9 位偏移量。
我们构建多级页表的目标：`使页表的每一部分都能放入一个页`.
到目前为止，我们只考虑了页表本身。但是，如果`页目录太大`，该怎么办? (之前是页表太大.)

要确定多级表中`需要多少级别`才能使页表的所有部分都能放入一页, 首先要确定多少页表项可以放入一页. 
鉴于物理页大小为 512 字节，并且假设 PTE 大小为 4 字节，你应该看到，可以在单个页上放入 128 个 PTE. (1个物理页帧放128个页表项)
当我们索引页表时，我们可以得出结论，我们需要 VPN的最低有效位 7 位（log2 128）作为索引：

- ![](assets/Pasted%20image%2020230402183354.png)

光页目录就用了14位. 如果我们的`页目录有2^14个目录项`, 那么它不是一个页, 而是128个, 因此我们让多级页表的每一个部分放入一页目标失败了.

为了解决这个问题，我们为树再加一层，将`页目录项本身再拆成多个页`，然后在其上添加另一个页目录，`指向页目录项的页`。我们可以按如下方式分割虚拟地址：

- ![](assets/Pasted%20image%2020230402183556.png)
    - 现在，当索引上层页目录时，我们使用虚拟地址的最高几位（图中的 PD 索引 0）。该索引用于从顶级页目录中获取页目录项。如果有效，则通过组合来自顶级 PDE 的物理帧号和 VPN 的下一部分（PD 索引 1)来查阅页目录的第二级。最后，如果有效，则可以通过使用与第二级 PDE 的地址组合的页表索引来形成 PTE 地址。这会有很多工作。所有这些只是为了在多级页表中查找某些东西。

#### 地址转换过程：记住 TLB

为了总结使用两级页表的地址转换的整个过程，我们再次以算法形式展示控制流（见图 20.4）.
```c
1 VPN = (VirtualAddress & VPN_MASK) >> SHIFT
2 (Success, TlbEntry) = TLB_Lookup(VPN)
3 if (Success == True) // TLB Hit
4     if (CanAccess(TlbEntry.ProtectBits) == True)
5         Offset = VirtualAddress & OFFSET_MASK
6         PhysAddr = (TlbEntry.PFN << SHIFT) | Offset
7         Register = AccessMemory(PhysAddr)
8     else
9         RaiseException(PROTECTION_FAULT)
10 else // TLB Miss
11     // first, get page directory entry
12     PDIndex = (VPN & PD_MASK) >> PD_SHIFT
13     PDEAddr = PDBR + (PDIndex * sizeof(PDE))
14     PDE = AccessMemory(PDEAddr)
15     if (PDE.Valid == False)
16          RaiseException(SEGMENTATION_FAULT)
17     else
18         // PDE is valid: now fetch PTE from page table
19         PTIndex = (VPN & PT_MASK) >> PT_SHIFT
20         PTEAddr = (PDE.PFN << SHIFT) + (PTIndex * sizeof(PTE))
21         PTE = AccessMemory(PTEAddr)
22         if (PTE.Valid == False)
23             RaiseException(SEGMENTATION_FAULT)
24         else if (CanAccess(PTE.ProtectBits) == False)
25             RaiseException(PROTECTION_FAULT)
26         else
27             TLB_Insert(VPN, PTE.PFN, PTE.ProtectBits)
28             RetryInstruction()
```

### 20.4 反向页表

在反向页表（inverted page table）中，可以看到页表世界中更极端的空间节省。在这里，我们保留了一个页表，其中的项代表系统的每个物理页，而不是有许多页表（系统的每个进程一个）。页表项告诉我们哪个进程正在使用此页，以及该进程的哪个虚拟页映射到此物理页。

现在，要找到正确的项，就是要搜索这个数据结构。线性扫描很消耗性能，因此通常在此基础结构上建立散列表，以加速查找。

更一般地说，反向页表说明了我们从一开始就说过的内容：页表只是数据结构。你可以对数据结构做很多疯狂的事情，让它们更小或更大，使它们变得更慢或更快。

多层和反向页表只是人们可以做的很多事情的两个例子。

### 20.5 将页表交换到磁盘

到目前为止，我们一直假设页表位于内核拥有的物理内存中。即使我们有很多技巧来减小页表的大小，但是它仍然有可能是太大而无法一次装入内存。因此，一些系统将这样的页表放入内核虚拟内存（kernel virtual memory），从而允许系统在内存压力较大时，将这些页表中的一部分交换(swap)到磁盘。我们将在下一章(即 VAX/VMS 的案例研究)中进一步讨论这个问题，在我们更详细地了解了如何将页移入和移出内存之后。

### 20.6 小结

现在已经看到了如何构建真正的页表。不一定只是线性数组，而是更复杂的数据结构.

这样的页表体现了时间和空间上的折中(表格越大，TLB 未命中可以处理得更快，反之亦然)，因此结构的正确选择强烈依赖于给定环境的约束。

在一个内存受限的系统中(像很多旧系统一样)，小结构是有意义的。在具有较多内存，并且工作负载主动使用大量内存页的系统中，用更大的页表来加速 TLB 未命中处理，可能是正确的选择.

## Ch21 超越物理内存: 机制(Swap)

到目前为止, 我们一直`假定`虚拟地址空间非常小, 能放入物理内存.
事实上, 我们假设每个正在运行的进程的地址空间都能放入内存.
现在将放宽这些大的假设，并假设需要支持许多进程同时运行的巨大地址空间.

为了达到这个目的, 需要在`内存层级`(memory hierarchy)上再加一层.
到目前为止, 我们一直假设所有页都常驻在物理内存中.
但是, 为了支持更大的地址空间, OS需要把`当前没有在用的`那部分地址空间`找个地方存储起来`. 
一般来说, 这个地方有一个特点, 那就是比内存有更大的容量. 因此, 一般来说也更慢. 
在现代系统中, 硬盘(hard disk drive)通常能够满足这个需求。
因此, 在我们的存储层级结构中, 大而慢的硬盘位于底层, 内存在上. 那么我们的关键问题是:

关键问题：如何超越物理内存
操作系统如何利用大而慢的设备，透明地提供巨大虚拟地址空间的假象？

为什么我们要为进程支持巨大的地址空间？
答案还是`方便和易用性`。有了巨大的地址空间，你不必担心程序的数据结构是否有足够空间存储，只需自然地编写程序，根据需要分配内存。
这是OS提供的一个强大的假象.

不仅是一个进程, 增加`交换空间`让OS为`多个并发运行的进程`都提供巨大地址空间的假象.
`多道程序的出现`, `强烈要求能够换出一些页`. 因为早期的机器显然不能将所有进程需要的所有页同时放在内存中.
因此, 多道程序和易用性都需要OS支持比`物理内存`更大的地址空间. 
这是`所有现代虚拟内存系统`都会做的事情, 也是现在我们要进一步学习的内容.

### 21.1 交换空间

我们要做的`第一件事情`就是, 在`硬盘上开辟一部分空间用于物理页的移入和移出`.
在OS中, 一般这样的空间称为`交换空间(swap space)`. 因为我们将内存中的页交换到其中，并在需要的时候又交换回去.
因此, 我们会假设`OS`能够`以页大小为单元`读取或者写入交换空间.
为了达到这个目的，操作系统`需要记住给定页的硬盘地址(disk address)`.

`交换空间的大小`是非常重要的, 它决定了系统在某一时刻能够使用的最大内存页数.
简单起见, 现在假设它非常大.

- ![](assets/Pasted%20image%2020230402203337.png)

如上图, 有一个4页的物理内存和一个8页的交换空间.
在这个例子中, 3 个进程(进程 0, 进程 1 和进程 2)主动共享物理内存.
每个进程都只有一部分有效页在内存中, 剩下的页都在硬盘的交换空间中.
第 4 个进程（进程 3）的所有页都被交换到硬盘上，因此很清楚它目前没有运行。
有一块交换空间是空闲的。
这个例子就能清楚, swap space让系统内存好像比实际物理内存更大.

注意, `交换空间不是唯一的硬盘交换目的地`.
例如, 假设运行一个`二进制程序`(如ls, 或你写的main程序). 这个`二进制程序的代码页`最开始是在`硬盘`上, 但程序运行的时候, 它们被`加载到内存`中(要么在程序开始运行时全部加载, 要么在现代操作系统中, 按需要一页一页加载). 但是, 如果系统需要在物理内存中腾出空间以满足其他需求，则可以安全地重新使用这些代码页的内存空间, 因为稍后它又可以重新从硬盘上的`二进制文件`加载.

### 21.2 存在位

现在我们在硬盘上有一些空间, 需要在系统中增加一些`更高级的机制`, 来`支持从硬盘交换页`. 简单起见，假设有一个`硬件管理` TLB 的系统.

先回想一下`内存引用`发生了什么:

正在运行的进程生成`虚拟内存引用`(用于获取指令或访问数据), 在这种情况下, 硬件将其`转换为物理地址`, 再从内存中获取所需数据. 

硬件(MMU)首先从`虚拟地址获得 VPN`，检查 TLB 是否匹配（TLB 命中），如果命中，则获得最终的物理地址并从内存中取回。这希望是常见情形，因为它很快（不需要额外的内存访问）。

如果在 TLB 中找不到 VPN（即 TLB 未命中），则硬件在内存中查找页表（使用页表基址寄存器），并使用 VPN 查找该页的页表项（PTE）作为索引。如果页有效且存在于物理内存中，则硬件从 PTE 中获得 PFN, 将其插入 TLB，并重试该指令，这次产生 TLB 命中。

但是, 如果`希望允许页交换到硬盘`, 必须添加`更多的机制`. 
具体来说, 当`硬件在页表项中查找时`, 可能发现`页不在物理内存`中.
硬件(或OS, 在软件管理TLB 时)`判断是否在内存中`的方法, 是通过`页表项中的一条新信息`，即`存在位(present bit)`. 
如果`存在位设置为1`, 则表示该页`存在于物理内存`中, 并且所有内容都如上所述进行.
如果`存在位设置为0`, 则页`不在物理内存`中, 而在硬盘上. 访问不在物理内存中的页, 这种行为通常被称为`页错误(page fault)`. 也叫`缺页异常`.

在页错误时，操作系统被唤起来处理页错误。一段称为“`页错误处理程序`(page-fault handler)"的代码会执行，来处理页错误，接下来就会讲.

### 21.3 页错误(缺页)

已经介绍过在`TLB未命中`的情况下, 我们有两种类型的系统: `硬件管理的TLB`(硬件在页表中找到需要的转换映射)和`软件管理的TLB`(操作系统执行查找过程).

`不论`在哪种系统中, 如果`页不存在`, 都`由OS负责`处理`页错误`. 操作系统的`页错误处理程序`(page-fault handler)确定要做什么. 几乎所有的系统都在`软件中`处理页错误. 
即使是硬件管理的TLB, 硬件也信任OS来管理这个重要的任务.

如果一个页不存在, 它`已被交换到硬盘`, 在处理页错误的时候, `OS需要将该页交换到内存中`.

问题来了：`OS如何知道所需的页在哪儿?`
在许多系统中, `页表`是存储这些信息最自然的地方.
因此，OS可以用`PTE中的某些位`来存储`硬盘地址`, 这些位`通常用来存储像页的PFN这样的数据`. 当操作系统接收到页错误时, 它会在`PTE中`查找`地址`, 并将`请求发送到硬盘`, 将页读取到内存中. 

(页表项中`原来存物理页号的地方`, 也可以用来`存储在硬盘上的地址`, `配合存在位`.)

>补充: 为什么硬件不能处理页错误?
>首先, 页错误导致的`硬盘操作很慢`. 即使OS需要很长时间来处理故障, 执行大量的指令, 但相比于硬盘操作, 这些额外开销是很小的. 
>其次, 为了能够处理页故障, 硬件必须了解交换空间, 如何向硬盘发起I/O操作, 以及很多它当前所不知道的细节. 因此, 由于性能和简单的原因, `OS来处理页错误`, 这样硬件人员也很开心.

当`硬盘I/O完成时`, OS会`更新页表`, 将此页标记为`存在`, `更新`页表项(PTE)的`PFN字段`以记录新获取页的内存位置, 并`重试指令`.
下一次`重新访问TLB`还是未命中, 然而这次因为`页在内存中`, 因此会`将页表中的地址更新到TLB中`(也可以在处理页错误时更新TLB以避免此步骤).
最后的重试操作会`在 TLB 中找到转换映射`, 从已转换的内存物理地址, 获取所需的数据或指令.
(`Swap space -> 内存 -> TLB`).

注意, 当 `I/O 在运行时`, `进程`将处于`阻塞(blocked)状态`. 因此, 当页错误正常处理时, OS可以自由地运行其他可执行的进程.
因为I/O操作时间较长, 一个进程进行I/O(页错误)时会执行另一个进程, 这种`交叠(overlap)`是多道程序系统充分利用硬件的一种方式.(交叉运行)

### 21.4 内存满了怎么办

在上面描述的过程中, 你可能会注意到, 都是假设有足够的空闲内存来从存储交换空间换入(page in)的页.
当然, 情况可能并非如此. `内存可能已满`(或接近满了).
因此, `OS`可能希望`先交换出(page out)一个或多个页`, 以便为OS即将交换入的新页留出空间. 

选择哪些页被交换出或被替换(replace)的过程, 被称为`页交换策略(page-replacement policy)`.

若`换出不合适的页`会导致程序性能上的巨大损失, 也会导致程序以类似硬盘的速度运行而不是以类似内存的速度. 在现有的技术条件下，这意味着程序可能会运行慢 10000～100000 倍.
因此, 这样的策略是值得详细研究的.  后面会继续介绍.

### 21.5 页错误处理流程

我们现在可以粗略地描绘内存访问的完整流程.
代码21.2 展示了硬件在地址转换过程中所做的工作, 21.3展示了OS在页错误时所做的工作.

```c
1 VPN = (VirtualAddress & VPN_MASK) >> SHIFT
2 (Success, TlbEntry) = TLB_Lookup(VPN)
3 if (Success == True) // TLB Hit
4     if (CanAccess(TlbEntry.ProtectBits) == True)
5         Offset = VirtualAddress & OFFSET_MASK
6         PhysAddr = (TlbEntry.PFN << SHIFT) | Offset
7         Register = AccessMemory(PhysAddr)
8     else
9         RaiseException(PROTECTION_FAULT)
10 else // TLB Miss
11     PTEAddr = PTBR + (VPN * sizeof(PTE))
12     PTE = AccessMemory(PTEAddr)
13     if (PTE.Valid == False)
14         RaiseException(SEGMENTATION_FAULT)
15     else
16         if (CanAccess(PTE.ProtectBits) == False)
17             RaiseException(PROTECTION_FAULT)
18         else if (PTE.Present == True)
19              // assuming hardware-managed TLB
20             TLB_Insert(VPN, PTE.PFN, PTE.ProtectBits)
21             RetryInstruction()
22         else if (PTE.Present == False)
23             RaiseException(PAGE_FAULT)
```
代码21.2 页错误控制流算法（硬件部分）

从21.2的硬件控制流图中, 可以注意到当 TLB 未命中发生的时候有3 种重要情景:

- 第一种情况, 该页`存在`(present)且`有效`(valid)(第 18～21 行). 在这种情况下, TLB 未命中处理程序可以简单地从 PTE 中获取 PFN, 然后重试指令(这次 TLB 会命中), 并因此继续前面描述的流程.

- 第二种情况(第 22～23 行), 页错误(缺页)处理程序需要运行. 虽然这是进程可以访问的`合法页`(毕竟是有效的), 但它并`不在物理内存中`.

- 第三种情况, 访问的是一个`无效页`, 可能由于程序中的错误(第 13～14 行). 在这种情况下, PTE 中的其他位都不重要了. 硬件捕获这个非法访问, 操作系统陷阱处理程序运行, 可能会杀死非法进程.

```c
1 PFN = FindFreePhysicalPage()
2 if (PFN == -1) // no free page found
3     PFN = EvictPage() // run replacement algorithm
4 DiskRead(PTE.DiskAddr, pfn) // sleep (waiting for I/O)
5 PTE.present = True // update page table with present
6 PTE.PFN = PFN // bit and translation (PFN)
7 RetryInstruction() // retry instruction
```
代码 21.3 页错误控制流算法(软件部分)

从21.3 的软件控制流中, 可以看到为了处理页错误, OS大致做了什么:
首先, `OS必须`为将要换入的页`找到一个物理页帧`, 如果没有这样的物理帧, 我们将不得不等待交换算法运行, 并从内存中换出一些页到硬盘, 释放帧供这里使用. 
在获得物理帧后, 处理程序`发出I/O请求`从交换空间读取页.
最后, 当这个耗时的操作完成时, OS更新页表并重试指令. 重试将导致TLB未命中, 然后再一次重试时, TLB 命中, 此时硬件将能够访问所需的值.

### 21.6 交换何时真正发生

到目前为止, 我们一直描述的是OS会`等到内存已经完全满了`以后才会执行交换流程, 然后才替换(踢出)一个页为其他页腾出空间.
其实, OS会更`主动地预留一小部分空闲内存`.

为了保证有少量的空闲内存, 大多数OS会设置`高水位线(High Watermark,HW)`和`低水位线(Low Watermark,LW),` 来`帮助决定何时`从内存中清除页.
原理是这样: 
当OS发现有少于LW个页可用时, `后台`负责释放内存的`线程`会唤醒运行, 直到有HW个可用的物理页.

这个`后台线程`有时称为`交换守护进程(swap daemon)`或`页守护进程(page daemon)`, 它然后会进入休眠状态, 因为它毕竟为OS释放了一些内存.

通过`同时执行多个交换过程`, 可以进行一些`性能优化`.
例如, 许多系统会把`多个要写入`的页 `聚集`(cluster)或`分组`(group), `同时写入`到交换区间, 从而`提高硬盘的效率`. (多量少次)
这种合并操作`减少了硬盘的寻道和旋转开销`, 从而显著提高了性能.

为了配合后台的分页线程, 代码21.3 中的控制流需要稍作修改.` 交换算法`需要`先简单检查是否有空闲页`, 而不是直接执行替换.
如果没有空闲页, 会通知后台`分页线程`按需要`释放页`. 当`线程释放一定数目的页时`, 它会`重新唤醒原来的线程`, 然后就可以把需要的页交换进内存, 继续它的工作.

> 提示: 把工作放后台
> 当你有一些工作要做的时候, 可把工作放在`后台`(background)运行, 可以提高效率, 并允许将这些操作`合并执行`. OS通常在后台执行很多工作.

例如, 在将数据写入硬盘之前, 许多系统在`内存中缓冲要写入的数据`. 这样做有很多好处: 提高硬盘效率, 因为硬盘现在可以`一次写入多次要写入的数据`, 因此能够更好地调度这些写入. 优化了写入延迟, 因为数据写入到内存就可以返回. 可能减少某些操作, 因为写入操作可能不需要写入硬盘(例如, 如果文件马上又被删除), 也能更好地利用系统空闲时间(idle time), 因为系统可以在空闲时完成后台工作, 从而更好地利用硬件资源.

### 21.7 小结

本小章介绍了swap机制, 以使物理内存可以被更多的进程同时使用. 使系统可以访问超出物理内存大小限制的内存.

要做到这一点，在页表结构中需要添加额外信息，比如增加一个`存在位`(present bit，或者其他类似机制), 告诉我们该页是不是在物理内存中. 如果不存在，则操作系统页错误处理程序(page-fault handler)会运行以处理页错误(page fault), 从而将需要的页从硬盘读取到内存, 可能还需要先换出内存中的一些页，为即将换入的页腾出空间. 

这些行为`对进程都是透明的`. 对进程而言, 它只是访问自己私有的, 连续的虚拟内存. 至于谁给它提供的环境, 它不管.
在后台, `物理页`被`放置在物理内存中`的`任意(非连续)位置`, 有时它们甚至不在内存中, 需要从硬盘取回. 
虽然我们希望在一般情况下内存访问速度很快，但在某些情况下，它需要多个硬盘操作的时间。像执行单条指令这样简单的事情，在最坏的情况下，可能需要很多毫秒才能完成。

## Ch22 超越物理内存(swap): 策略(算法)

在虚拟内存管理程序中, 假如拥有大量空闲内存, 操作就会变得很容易. 
页错误(缺页)发生了, 你在空闲页列表中找到空闲页, 将它分配给不在内存中的页即可.

那当`内存不够`的时候呢?
在这种情况下, 由于`内存压力`(memory pressure)迫使操作系统`换出`(paging out)一些页, 为`常用的页`腾出空间.
确定要踢出(evict)哪个页(或哪些页)`封装`在OS的`替换策略(replacement policy)`中.

历史上, 这是早期的虚拟内存系统要做的`最重要的决定之一`, 因为旧系统的物理内存非常小. 
至少，有一些策略是非常值得了解的.

关键问题：如何决定踢出哪个页
OS如何决定从内存中踢出哪一页(或哪几页)? 这个决定由系统的替换策略做出, 替换策略通常会遵循一些通用的原则(下面将会讨论), 但也会包括一些调整, 以避免特殊情况下的行为.

### 22.1 缓存管理

在深入研究策略之前, 先详细描述一下我们要解决的问题. 
由于`内存`只包含系统中`所有页的子集`, 因此可以`将内存视为`系统中`虚拟内存页的缓存`(cache).
因此, 在为这个缓存选择替换策略时, 我们的目标是让`缓存未命中(cache miss)`最少, 就是`从磁盘获取页的次数最少`.
或者, 可以将目标看成让缓存命中(cache hit)最多, 即在内存中找到待访问页的次数最多.

知道了缓存命中和未命中的次数, 就可以计算程序的`平均内存访问时间`(Average Memory Access Time, AMAT, 计算机架构师衡量硬件缓存的指标).
具体来说, 给定这些值, 可以按照如下公式计算`AMAT`：

- ![](assets/Pasted%20image%2020230403151755.png)

其中`TM`表示`访问内存`的时间, `TD `表示`访问磁盘`的时间, `PHit`表示在缓存中找到数据的命中率, 
`PMiss`表示在缓存中找不到数据的概率(未命中). `PHit和 PMiss从0.0变化到1.0`, 并且`PMiss+PHit =1.0`.

例如, 假设有一个机器有小型地址空间:4KB(2^12), 每页 256 字节(2^8).
因此，虚拟地址由两部分组成：一个4位VPN(最高有效位)和一个 8 位偏移量(最低有效位).
因此，本例中的一个进程可以访问总共2^4=16个虚拟页. 
在这个例子中，该进程将产生以下内存引用（即虚拟地址）0x000，0x100，0x200，0x300，0x400，0x500，0x600，0x700，0x800，0x900。
这些虚拟地址指向地址空间中前 10 页的每一页的第一个字节（页号是每个虚拟地址的第一个十六进制数字）。

让我们进一步假设，除了虚拟页 3 之外，所有页都已经在内存中。
命中率（hit rate，在内存中找到引用的百分比）：90%（PHit = 0.9）.

假设访问内存（TM）的成本约为 100ns，并且访问磁盘（TD）的成本大约为 10ms，则我们有以下 AMAT：0.9×100ns+0.1×10ms，即90ns + 1ms或1.0009ms，或约1ms。如果我们的命中率是99.9%（PMiss = 0.001），结果是完全不同的：AMAT 是 10.1μs，大约快 100 倍。当命中率接近 100%时，AMAT 接近100ns。

在现代系统中，磁盘访问的成本非常高，即使`很小概率的未命中`也会拉低正在运行的程序的总体 AMAT.
显然，我们必须`尽可能地避免缓存未命中`，避免程序以磁盘的速度运行。要做到这一点，有一种方法就是仔细开发一个聪明的策略，像我们现在所做的一样.

### 22.2 最优替换策略

`最优替换策略`(The Optimal Replacement Policy)能达到`总体未命中数量最少`.
即替换内存中在`最远将来`才会被访问到的页, 可以达到缓存未命中率最低.(`替换最迟访问的`)

>虽然最优策略非常不切实际, 但作为`仿真`或其他研究的比较者还是非常有用的.
>因此，在你进行的任何研究中，知道最优策略可以方便进行对比，知道你的策略有多大的改进空间，也用于决定当策略已经非常接近最优策略时，停止做无谓的优化.

理解最优策略背后的想法。这样想：如果你不得不踢出一些页, 为什么不踢出在将来最迟才会访问的页呢？这样做基本上是说, 缓存中所有其他页都比这个页重要.
道理很简单: 在引用最迟将来会访问的页之前, 你肯定会引用其他页。(最近需要访问的页留着)

追踪一个简单的例子，来理解最优策略的决定。假设一个程序按照以下顺序访问虚拟页：0，1，2，0，1，3，0，3，1，2，1。表 22.1 展示了最优的策略，这里假设缓存可以存 3 个页.

- ![](assets/Pasted%20image%2020230403172545.png)

前 3 个访问是未命中，因为缓存开始是空的. 这种未命中有时也称作`冷启动未命中`（cold-start miss，或强制未命中，compulsorymiss）.
当缓存了0,1,2时已经满了, 遇到3时, 要决定把哪一个页换到硬盘上去.
根据最优替换算法, 接下来, 最后才访问的是页2. 所以把页2替换掉. 等后面再访问2时, 又未命中, 
0,1,3 决定换出哪个页. 再根据最优替换算法, 只要不替换1就行, 0和3都可以. 

> 补充：缓存未命中的类型
> 在计算机体系结构世界中，架构师有时会将未命中分为 3 类：强制性、容量和冲突未命中，有时称为3C.
> 强制性(compulsory miss)未命中(或冷启动未命中, cold-start miss)是因为缓存开始是空的, 而这是对项目的第一次引用.
> 容量未命中(capacity miss): 由于缓存的空间不足而不得不踢出一个项目以将新项目引入缓存
> 冲突未命中(conflict miss): 出现在硬件中，因为硬件缓存中对项的放置位置有限制，这是由于所谓的集合关联性（set-associativity）。
> 它不会出现在OS页面缓存中，因为这样的缓存总是完全关联的（fully-associative），即对页面可以放置的内存位置没有限制.

计算缓存命中率: 有6次命中和5次未命中, 那么缓存命中率Hits/(Hits+Misses) 就是54.5%.
如果除去强制未命中, 那命中率就是81.8%.
遗憾的是, 正如我们之前在开发CPU调度策略时所看到的那样, `未来的访问是无法知道的`, 你`无法`为通用OS`实现最优策略`.
因此，在开发一个真正的、可实现的策略时，我们将聚焦于寻找其他决定把哪个页面踢出的方法.因此, `最优策略只能作为比较`, 知道我们的策略有多接近“完美”.

### 22.3 简单策略：FIFO

早期的系统避免了尝试达到最优的复杂性, 而采用了非常简单的替换策略.
例如, 一些系统使用FIFO(先入先出)替换策略. 页在进入系统时，简单地放入一个队列。当发
生替换时，队头的页（“先入”页）被踢出.
FIFO 有一个很大的优势：实现起来相当简单.

- ![](assets/Pasted%20image%2020230403175644.png)
- 对比 FIFO 和最优策略，FIFO 明显不如最优策略，FIFO 命中率只有 36.4%（不包括强制性未命中为 57.1%）。
- 先进先出（FIFO)根本无法确定页的重要性：即使页 0 已被多次访问，FIFO 仍然会将其踢出，因为它是第一个进入内存的.

### 22.4 另一简单策略：随机

在内存满的时候它`随机选择`一个页进行替换. 类似于FIFO, 实现起来很简单, 但是它在挑选替换哪个页时不够智能.

- ![](assets/Pasted%20image%2020230403181446.png)

当然，随机的表现完全取决于多幸运.
在上面的例子中，随机比 FIFO 好一点，比最优的差一点。事实上，我们可以运行数千次的随机实验，求得一个平均的结果。图 22.1显示了 10000 次试验后随机策略的平均命中率，每次试验都有不同的随机种子。正如你所看到的，有些时候（仅仅 40%的概率），随机和最优策略一样好，在上述例子中，命中内存的次数是 6 次。有时候情况会更糟糕，只有 2 次或更少。随机策略取决于当时的运气。

- ![](assets/Pasted%20image%2020230403181551.png)

### 22.5 利用历史数据：LRU

FIFO跟随机两个算法, 有个共同问题, 就是会踢出马上要访问的页.
FIFO, Random和类似的策略不太可能达到最优, 需要更智能的策略.

正如在调度策略所做的那样, 为了提高后续的命中率, 我们再次通过历史的访问情况作为参考.
例如, 如果某个程序在过去访问过某个页, 则很有可能在不久的将来会再次访问该页.(局部性)

页替换策略可以使用一个`历史信息`, `频率(frequency)`.
如果一个页被访问了很多次, 也许它不应该被替换, 因为它显然更有价值. 页更常用的属性是访问的`近期性(recency)`, 越近被访问过的页, 也许再次访问的可能性也就越大. 

这一系列的策略是基于人们所说的`局部性原则(principle of locality)`. 基本上只是对程序及其行为的观察. 
这个原理简单地说就是程序倾向于频繁地访问某些代码(例如循环)和数据结构(例如循环访问的数组).
因此, 我们应该尝试用历史数据来确定哪些页面更重要, 并在需要踢出页时将这些页保存在内存中.

因此，一系列简单的基于历史的算法诞生了.
“`最不经常使用`”（Least-Frequently-Used,LFU）策略会替换最不经常使用的页。
“`最少最近使用`”（Least-Recently-Used,LRU）策略替换最近最少使用的页面。

为了更好地理解 LRU，我们来看看 LRU 如何在示例引用序列上执行. 表 22.4 展示了结果。

- ![](assets/Pasted%20image%2020230403183239.png)

在我们的简单例子中, LRU 的表现几乎快要赶上最优策略了. 

### 22.6 工作负载示例

让我们再看几个例子, 以便更好地理解这些策略.
我们将查看更复杂的`工作负载(workload)`, 而不是追踪小例子. 但是, 这些工作负载也被大大简化了. 更好的研究应该包含应用程序追踪. 

第一个工作负载`没有局部性`, 这意味着每个引用都是访问一个`随机`页.
在这个简单的例子中，工作负载每次访问独立的 100 个页，随机选择下一个要引用的页。
总体来说，访问了10000 个页。在实验中，我们将缓存大小从非常小（1 页）变化到足以容纳所有页（100页），以便了解每个策略在缓存大小范围内的表现。

图 22.2 展示了最优, LRU, 随机和FIFO策略的实验结果. 图 22.2 中的 y 轴显示了每个策略的命中率。如上所述，x 轴表示缓存大小的变化.

- ![](assets/Pasted%20image%2020230403183942.png)

我们可以从图 22.2 中得出一些结论。首先，当工作负载`不存在局部性时`，使用的策略区别不大。
LRU、FIFO 和随机都执行相同的操作，命中率完全由缓存的大小决定。
其次，当缓存大到可以存所有的页时, 使用哪种策略也无所谓，所有的策略都有 100%的命中率。
最后，你可以看到，最优策略的表现明显好于实际的策略。如果有可能的话，偷窥未来，就能做到更好的替换。

第二个`工作负载`就是所谓的“`82原则`”负载场景, 它表现出局部性: 80%的引用是访问20%的页(“热门”页). 剩下的 20%是对剩余的 80%的页（“冷门”页）访问.
在我们的负载场景，总共有 100 个不同的页。因此，“热门”页是大部分时间访问的页，其余时间访问的是“冷门”页。

- ![](assets/Pasted%20image%2020230403184411.png)

从图 22.3 中可以看出, 除了最优策略, `LRU表现的最好`, 因为它更可能保持热门页.

由于这些页面过去经常被提及，它们很可能在不久的将来再次被提及。最优策略再次表现得最好，表明 LRU 的历史信息并不完美。

你现在可能会想: LRU相对随机和FIFO的提升真的那么重要么. 答案是“看情况”.
如果每次未命中代价非常大(并不少见), 那么即使小幅提高命中率也会对性能产生巨大的影响.
如果未命中的代价不那么大，那么 LRU 带来的好处就不会那么重要.

最后一个workload.
我们称之为“`循环顺序`”工作负载，其中依次引用 50个页，从 0 开始，然后是 1，…，49，然后循环，重复访问，总共有 10000 次访问 50 个单独页。图 22.4 展示了这个工作负载下各个策略的行为。

- ![](assets/Pasted%20image%2020230403185650.png)

这种工作负载在许多应用程序(如数据库)中非常常见, 结果`LRU跟FIFO是最差情况.`
在循环顺序的工作负载下, 这些算法踢出较旧的页.
由于工作负载的循环性质, 这些较旧的页将比因为算法保存在缓存中的页更早被访问.
事实上，即使缓存的大小是49页，50 个页面的循环连续工作负载也会导致 0%的命中率.

### 22.7 实现基于历史信息的算法

正如你所看到的, 像LRU这样的算法通常优于简单的策略(如 FIFO 或随机), 简单策略可坑会踢出重要的页.
应该如何实现`基于历史的算法`呢？

以 LRU 为例. 为了实现它, 我们需要做很多工作.
具体地说，在`每次页访问`(即每次内存访问, 不管是取指令还是加载指令还是存储指令)时, 我们都`必须更新一些数据`, 从而`将该页移动到列表的前面` (即 `MRU 侧`).
与 FIFO 相比, FIFO 的页列表仅在页被踢出(通过移除最先进入的页)或者当新页添加到列表(已到列表尾部)时才被访问.
为了`记录`哪些页是最少和最近被使用, 系统必须对每次内存引用做一些记录工作. 显然, 如果不十分小心, 这样的记录反而会极大地影响性能.

有一种方法有助于加快 速度，就是增加一点`硬件支持.`
例如，硬件可以在`每个页访问时更新`内存中的`时间字段`（时间字段可以在每个进程的页表中，或者在内存的某个单独的数组中，每个物理页有一个）。当页被访问时，时间字段将被硬件设置为当前时间。
然后，在需要替换页时，`操作系统`可以简单地`扫描`系统中`所有页的时间字段`以找到`最近最少使用的页`.

但是, 随着系统中`页数量的增长`,` 遍历所有页`的时间字段只是为了找到最精确最少使用的页，`太消耗性能了`。
4GB 内存的机器,  一页4KB size. 有100万多个物理页帧. 要找到LRU页也`需要很长时间`.

这就引出了一个问题：我们是否真的需要找到绝对最旧的页来替换？找到差不多最旧的页可以吗?

关键问题: 如何实现LRU替换策略
由于实现完美的 LRU 代价非常昂贵，我们能否实现一个`近似的 LRU 算法`，并且依然能够获得预期的效果？

### 22.8 近似 LRU 算法(时钟算法)

许多现代系统的做法, 就是`近似LRU.`
这个想法需要硬件寄存器增加一个`使用位`(`use bit`, 有时称为`引用位`, `reference bit`).
系统的`每个页`有一个`使用位`, 然后这些使用位`存储在某个地方`. (例如, 它们可能在每个进程的页表中, 或者只在某个数组中).
每`当页被引用`(即读或写)时, `硬件`将使用位`设置为1`. 
但是, `硬件不会清除该位`(即将其设置为 0), 这由`OS来干`.

OS如何利用`使用位`来实现`近似LRU`?
有一个简单的方法称作`时钟算法`(clock algorithm). 
想象一下, 系统中的所有页都放在一个循环列表中. 时钟指针(clock hand)开始时指向某个特定的页(哪个页不重要). 
当必须进行页替换时, OS检查当前指向的页P的使用位是1还是0. 如果是1, 则意味着页面P最近被使用, 因此不适合被替换. 然后, P的使用位设置为0, 时钟指针递增到下一页(P+1).
该算法一直持续到找到一个使用位为0的页, 使用位为0意味着这个页最近没有被使用过(在最坏的情况下, 所有的页都已经被使用了, 那么就将所有页的使用位都设置为0).

注意, 这种方法不是通过使用位来实现近似 LRU 的唯一方法.
实际上, 任何周期性地清除使用位, 然后通过区分使用位是1和0来判定该替换哪个页的方法都是可以的.
时钟算法只是一个早期成熟的算法, 并且具有`不重复扫描`内存来寻找未使用页的特点, 也就是它在最差情况下, 只会遍历一次所有内存. 

图 22.5 展示了`时钟算法`的一个变种的性能.

- ![](assets/Pasted%20image%2020230403200443.png)

该变种在需要进行页替换时`随机扫描`各页, 如果遇到一个页的引用位为1, 就清除该位(即将它设置为0). 直到找到一个使用位为0的页, 将这个页进行替换.
如你所见, 虽然时钟算法不如完美的LRU做得好, 但它比不考虑历史访问的方法要好.

### 22.9 考虑脏页

时钟算法的一个小修改, 是对内存中的页是否被修改的额外考虑.
这样做的原因是: 如果页已被修改(modified)并因此变脏(dirty), 则必须将该页写回磁盘, 以达到踢出的目的, 这个行为开销较大.
如果它没有被修改(因此是干净的, clean), 踢出就没成本. 物理帧可以简单地重用于其他目的而无须额外的I/O.
因此, 一些虚拟机系统更倾向于`踢出干净页`, 而不是脏页. 

为了支持这种行为, 硬件应该包括一个`修改位`(modified bit, 又名脏位, dirty bit).
每次写入页时都会设置此位, 因此可以将其合并到页面替换算法中. 
例如, 时钟算法可以改成去找使用位为0(最近未使用), 脏位为0(干净的)的页, 踢出. 若无法找到这种页时, 再查找脏的, 未使用的页面, 等等.

### 22.10 其他虚拟内存策略

页面替换不是虚拟内存子系统采用的唯一策略(尽管它可能是最重要的).
例如, OS还必须决定`何时`将页加载`内存`. 该策略也称为`页选择`(page selection)策略, 它向OS提供了一些不同的选项.

对于大多数页而言, OS只是使用`请求调页(demand paging)`, 意思是OS在`页被访问时`才将页`载入`内存中, “按需”即可.
当然, OS可能会预测某个页即将会被使用, 从而提前载入内存中. 这种行为被称为预取(prefetching), 只有在成功率够大时, 才会用这个方法. 
例如, 一些系统将假设如果代码页P被载入内存, 那么代码页P+1很可能很快被访问, 因此也应该被载入内存. 

另一个策略决定了OS如何把页写入磁盘. 当然, 它们可以一次写一个. 但许多系统会在内存中收集一些需要写的页, 然后以一种(更高效)的写入方式`一次性`将它们写入硬盘.
这种行为通常称为`聚集(clustering)写入`, 或者就是`分组写入(grouping)`, 这样做有效是因为硬盘驱动器的性质, 执行单次大的写操作, 比许多小的写操作更有效. (每次写的多, 写的次数少)

### 22.11 抖动

当内存就是被`超额请求`时, OS应该做什么, 这组运行中的进程的内存需求是否超出了可用物理内存? 在这种情况下, 系统将不断地进行换页, 这种情况有时被称为抖动(thrashing).

一些早期的操作系统有一组相当复杂的机制, 以便在抖动发生时检测并应对.
例如, 假设有一组进程, 系统可以决定`不运行`部分进程, 以期望通过减少这些进程运行(它们活跃使用的页面)来适应内存, 从而可以改善.
这种方法通常被称为准入控制(admission control), 表明有时候, 少做点工作, 而且做的好, 比尝试一次性做所有事, 但做的很烂, 要好的多.

目前的一些系统采用更严格的方法处理内存过载. 例如, 当内存请求超过可用内存时，某些版本的 Linux 会运行“内存不足的杀手程序(out-of-memory killer)”. (oom) 这个守护进程选择一个内存占用大的进程并干掉它, 从减少内存占用. 
虽说减轻了内存压力, 但这种方法也有问题, 比如, 如果干掉X服务(图形界面的服务), 那就会导致所有需要显示的应用程序没法用. 

### 22.12 小结

我们已经看到了许多页替换(和其他)策略的介绍, 这些策略是所有现代操作系统中虚拟内存子系统的一部分.
现代系统添加了对时钟等类LRU的算法一些调整. 例如, 扫描抗性(scan resistance)是许多现代算法的重要组成部分，如 ARC.
扫描抗性算法通常是类似 LRU 的, 但也试图避免 LRU 在最坏情况下的行为, 我们曾在循环顺序工作负载中看到这种情况。因此，页替换算法的发展仍在继续.

多年来, 由于内存访问和磁盘访问次数之间的差异增加, 这些算法的重要性降低了. 由于把页换到硬盘开销很大, 因此频换页成本太高. 简而言之, 不管你的替换算法多么好, 如果你频繁换页, 你的系统也会变得无法接受地慢.
所以, 过度换页的最佳解决方案往往很简单: 购买更多的内存.

然而, 最近更快的存储设备的创新(比如基于闪存的SSD和英特尔的傲腾)再次改变了这些性能, 导致了页替换算法的复兴.

## Ch23 实现虚拟内存系统

在结束对虚拟内存的学习之前, 我们来近距离看看整个虚拟内存系统是如何组在一起的. 我们已经了解了这个系统的关键要素: 包括诸多页表的设计, 与TLB的交互(有时候甚至完全由OS自己处理) 和决定哪些页留在内存, 哪些踢出去的策略.
然而, 还有许多其他特性来一起构成完整的虚拟内存系统, 包括 性能, 实用性和安全性的诸多特性,  
所以, 我们的关键问题是:
如何构建一个完整的虚拟内存系统
实现完整的虚拟内存系统需要哪些功能? 它们如何提升性能, 提高安全性, 或者其他方式改进系统.

我们会通过介绍两个系统来看看它们是怎么实现的. 第一个是最早的"现代"虚拟内存管理的例子, 
出现于20世纪70年代和80年代早期开发的VAX/VMS OS. 这个OS中有许多技术和方法沿用至今, 所以很值得学习. 有些思想, 就算过去50年, 依旧值得学习.
大多数其他领域(如物理)的人都知道这种思想, 但必须用技术驱动的学科(如计算机科学) 来描述.

第二个是Linux了. 原因显而易见了, Linux是广泛使用的OS, 可以有效地运行在像手机这样小且低功耗的系统上, 也可以在现代数据中心中发现的最具可扩展性的多核系统上运行。
因此, 它的虚拟内存系统必然是足够灵活来在这些不同场景下成功运行. 
我们将会讨论两个系统, 在一个完整内存管理中, 来说明如何把前面提到的概念联系在一起.

### 23.1 VAX/VMS 虚拟内存

数字设备公司（DEC）在 20 世纪 70 年代末推出了 VAX-11 小型机体系结构。
DEC在小型机时代是计算机行业的巨头. 但种种原因吧, 该公司倒闭了.
VAX-11/780 和功能较弱的 VAX-11/750 实现了这种架构.

该系统的操作系统被称为 VAX/VMS(或这就是VMS)，其主要架构师之一是 Dave Cutler，他后来领导开发了微软 Windows NT.
VMS面临通用性的问题, 即它运行在各种机器上, 从很便宜的VAXen, 到同一架构系列里极高端和强大的机器. 因此, 操作系统必须具有一些机制和策略, 适用于这一系列广泛的系统(并且运行良好).

多嘴一句, VMS是个很好的例子, 用软件的创新来隐藏体系结构的一些固有缺陷. 尽管OS经常依赖硬件来构建高效的抽象和假象, 而有时候硬件设计者并没有把任务完成的很好. 在VAX硬件上, 我们会看到一些这样的例子, 尽管有这些硬件的缺陷, VMS OS做的工作依旧能让系统高效的运行.

#### 用于内存管理的硬件

VAX-11给每个进程提供了一个32位的虚拟地址空间, 一页512字节(2^9). 虚拟页号是23位, 9位页内偏移量. 而且VPN的高2位用来区分页所在段. 所以这是一个段页式系统.

00 00 0000 0000 0000 0000 000 0000 0000 0
段 |       段内页                           | 页内偏移量

页表项都有2^21个.

地址空间的低地址的一半, 是进程空间, 对每个进程都是独一无二的. 在进程空间的前半部分(又称P0), 存放用户程序, 和向下生长的堆.  进程空间的后半部分(P1), 存放着向上生长的栈.
地址空间的高地址的一半, 就是系统空间(S), 系统空间也只用了一半. 受保护的操作系统代码和数据存放在此处, 操作系统以这种方式跨进程共享.

VMS 设计人员的一个主要关注点是 VAX 硬件中的页大小非常小（512 字节）. 历史原因选择这个大小, 造成了一个问题, 线性页表太大. 
所以, 设计者的第一个目标就是确保VMS不会被页表占满内存.

系统通过两种方式, 减少页表对内存的压力.
首先，通过将用户地址空间分成两部分，VAX-11 为每个进程的每个区域（P0 和 P1）提供了一个页表。因此，栈和堆之间未使用的地址空间部分不需要页表空间。
基址和界限寄存器的使用与你期望的一样. 一个基址寄存器保存该段的页表的地址, 界限寄存器保存其大小(即页表项的数量). 

其次，通过在内核虚拟内存中放置用户页表（对于 P0 和 P1，因此每个进程两个），操作系统进一步降低了内存压力。
因此，在分配或增长页表时，内核在段 S 中分配自己的虚拟内存空间。如果内存受到严重压力，内核可以将这些页表的页面交换到磁盘，从而使物理内存可以用于其他用途.

将页表放入内核虚拟内存意味着地址转换更加复杂. 
例如，要转换 P0 或 P1 中的虚拟地址，硬件必须首先尝试在其页表中查找该页的页表项（该进程的 P0 或 P1 页表）。但是，在这样做时，硬件可能首先需要查阅系统页表（它存在于物理内存中）。随着地址转换完成，硬件可以知道页表页的地址，然后最终知道所需内存访问的地址。幸运的是，VAX 的硬件管理的 TLB 让所有这些工作更快，TLB 通常（很有可能）会绕过这种费力的查找。

#### 一个真实的地址空间

研究 VMS的好处，是可以看到一个真正的地址空间是如何构建的.
到目前为止，我们一直假设了一个简单的地址空间，只有用户代码、用户数据和用户堆，
但正如我们上面所看到的，真正的地址空间显然更复杂。

- ![](assets/Pasted%20image%2020230404063220.png)

> 为什么空指针访问会导致段错误
> `int *p = NULL; // set p = 0`
  `*p = 10; // try to store value 10 to virtual address 0`
> 虚拟地址 0. 硬件试图在 TLB 中查找 VPN（这里也是 0），遇到 TLB 未命中。查询页表，并且发现 VPN 0 的条目被标记为无效。因此，我们遇到无效的访问，将控制权交给操作系统，这可能会终止进程.

例如，代码段永远不会从第 0 页开始。相反，该页被标记为不可访问，以便为检测空指针（null-pointer）访问提供一些支持。因此，设计地址空间时需要考虑的一个问题是对调试的支持，这正是无法访问的零页所提供的。

也许更重要的是，内核虚拟地址空间（即其数据结构和代码）是每个用户地址空间的一部分。
在上下文切换时，操作系统改变 P0 和 P1 寄存器以指向即将运行的进程的适当页表。但是，它不会更改 S 基址和界限寄存器，并因此将“相同的”内核结构映射到每个用户的地址空间。

内核映射到每个地址空间, 是有原因的. 这种结构使得内核的运行更方便.
例如，如果操作系统收到用户程序（例如，在 write()系统调用中）递交的指针，很容易将数据从该指针处复制到它自己的结构. 
操作系统自然是写好和编译好的，无须担心它访问的数据来自哪里。相反，如果内核完全位于物理内存中，那么将页表的交换页切换到磁盘是非常困难的。如果内核被赋予了自己的地址空间，那么在用户应用程序和内核之间移动数据将再次变得复杂和痛苦。通过这种构造（现在广泛使用），内核几乎就像应用程序库一样，尽管是受保护的。

关于这个地址空间的最后一点与保护有关.
显然，操作系统不希望用户应用程序读取或写入操作系统数据或代码。因此，硬件必须支持页面的不同保护级别才能启用该功能。
VAX 通过在页表中的保护位中指定 CPU 访问特定页面所需的特权级别来实现此目的。因此，
系统数据和代码被设置为比用户数据和代码更高的保护级别。试图从用户代码访问这些信
息，将会在操作系统中产生一个陷阱，并且可能会终止违规进程。

#### 页替换

VAX 中的页表项（PTE）包含以下位：一个有效位，一个保护字段（4 位），一个修改（或脏位）位，为 OS 使用保留的字段（5 位），最后是一个物理帧号码（PFN）将页的位置存储在物理内存中。
敏锐的读者可能会注意到：没有引用位（no reference bit）！因此，VMS 替换算法必须在没有硬件支持的情况下，确定哪些页是活跃的.

开发人员也担心会有“自私贪婪的内存”（memory hog）—— 一些程序占用大量内存，使其他程序难以运行。到目前为止，我们所看到的大部分策略都容易受到这种内存的影响。
例如，LRU 是一种全局策略，不会在进程之间公平分享内存。

为了解决这两个问题，开发人员提出了分段的 FIFO（segmented FIFO）替换策略. 
想法很简单：每个进程都有一个可以保存在内存中的最大页数，称为驻留集大小（Resident Set
Size，RSS）。每个页都保存在 FIFO 列表中。当一个进程超过其 RSS 时，“先入”的页被驱逐。FIFO 显然不需要硬件的任何支持，因此很容易实现。

为了提高 FIFO 的性能，VMS 引入了两个二次机会列表（second-chance list），页在从内存中被踢出之前被放在其中。具体来说，是全局的干净页空闲列表和脏页列表。当进程 P 超过其 RSS 时，将从其每个进程的 FIFO 中移除一个页。如果干净（未修改），则将其放在干净页列表的末尾。如果脏（已修改），则将其放在脏页列表的末尾。

如果另一个进程 Q 需要一个空闲页，它会从全局干净列表中取出第一个空闲页。但是，如果原来的进程 P 在回收之前在该页上出现页错误，则 P 会从空闲（或脏）列表中回收，从而避免昂贵的磁盘访问。这些全局二次机会列表越大，分段的 FIFO 算法越接近 LRU.

VMS 采用的另一个优化也有助于克服 VMS 中的小页面问题。具体来说，对于这样的小页面，交换过程中的硬盘 I/O 可能效率非常低，因为硬盘在大型传输中效果更好。
为了让交换 I/O 更有效，VMS 增加了一些优化，但最重要的是聚集（clustering）。通过聚集，VMS将大批量的页从全局脏列表中分组到一起，并将它们一举写入磁盘（从而使它们变干净）。
聚集用于大多数现代系统，因为可以在交换空间的任意位置放置页，所以操作系统对页分组，执行更少和更大的写入，从而提高性能。

#### 其他漂亮的虚拟内存技巧

VMS 有另外两个现在成为标准的技巧：按需置零(demand zeroing)和写入时复制(copy-on-
write)。我们现在描述这些惰性（lazy）优化。

VMS（以及大多数现代系统）中的一种懒惰形式是页的按需置零（demand zeroing）。
为了更好地理解这一点，我们来考虑一下在你的地址空间(在堆里)中添加一个页的例子.
在一个初级实现中，操作系统响应一个请求，在物理内存中找到页，将该页添加到你的堆中，并将其置零（安全起见，这是必需的。否则，你可以看到其他进程使用该页时的内容。）,然后
将其映射到你的地址空间（设置页表以根据需要引用该物理页）。但是初级实现可能是昂贵的，特别是如果页没有被进程使用。

利用按需置零，当页添加到你的地址空间时，操作系统的工作很少。它会在页表中放入一个标记页不可访问的条目。如果进程读取或写入页，则会向操作系统发送陷阱。在处理陷阱时，操作系统注意到（通常通过页表项中“保留的操作系统字段”部分标记的一些位），这实际上是一个按需置零页。此时，操作系统会完成寻找物理页的必要工作，将它置零，并映射到进程的地址空间。如果该进程从不访问该页，则所有这些工作都可以避免，从而体现按需置零的好处。

> 惰性可以使得工作推迟，但出于多种原因，这在操作系统中是有益的。
> 首先，推迟工作可能会减少当前操作的延迟，从而提高响应能力。例如，操作系统通常会报告立即写入文件成功，只是稍后在后台将其写入硬盘。其次，更重要的是，惰性有时会完全避免完成这项工作。例如，延迟写入直到文件被删除，根本不需要写入。

VMS有另一个很酷的优化（几乎每个现代操作系统都是这样），写时复制（copy-on-write，COW）。
它很简单：如果操作系统需要将一个页面从一个地址空间复制到另一个地址空间，不是实际复制它，而是将其映射到目标地址空间，并在两个地址空间中将其标记为只读. 如果两个地址空间都只读取页面，则不会采取进一步的操作，因此操作系统已经实现了快速复制而不实际移动任何数据。
但是，如果其中一个地址空间确实尝试写入页面，就会陷入操作系统。操作系统会注意到该页面是一个 COW 页面，因此（惰性地）分配一个新页，用修改的数据填充它，并将这个新页映映射到报错进程的地址空间. 然后该进程继续运行, 并且有了自己的新私有页副本了.

COW能有用, 是有一些原因的. 
当然，任何类型的共享库都可以通过写时复制，映射到许多进程的地址空间中，从而节省宝贵的内存空间。在 UNIX 系统中，由于 fork()和 exec()的语义，COW 更加关键。你可能还记得，fork()会创建调用者地址空间的精确副本。对于大的地址空间，这样的复制过程很慢，并且是数据密集的。更糟糕的是，大部分地址空间会被随后的exec()调用立即覆盖，它用即将执行的程序覆盖调用进程的地址空间。通过改为执行写时复制的 fork()，操作系统避免了大量不必要的复制，从而保留了正确的语义，同时提高了性能。

### 23.2 Linux 虚拟内存系统

我们现在来讨论一下Linux虚拟内存系统的一些有趣的方面. Linux通过工程师们解决那些在实际生产中遇到的问题而不断发展. 因此, 大量的功能已经慢慢地被整合到现在功能齐全, 功能丰富的虚拟内存系统中. 

然而我们无法讨论Linux虚拟内存的方方面面, 只能触及最重要的那些,  尤其是超越了那些如VAX/VMS的经典虚拟内存系统出现的内容. 同时我们也会尝试列出Linux和老系统的共同点.

这次讨论我们会聚焦在Intel X86架构上的Linux. 当然Linux可以在许多不同架构的处理器上运行, X86 是Linux最主要和最重要的部署, 因此也是我们关注的焦点.

#### Linux 地址空间

和其他现代操作系统一样, 当然也和VAX/VMS 一样, 一个LInux虚拟地址空间 由用户区(存放用户程序代码, 栈, 堆和其他部分) 和 内核区(存放内核代码, 栈, 堆和其他部分). 如同其他系统, 上下文切换时, 当前运行的地址空间的用户区会发生变化. 在各进程的内核区是一样的.
在用户模式运行的程序也不能访问内核虚拟页. 只有陷入内核并且转换到特权模式才能访问内核的内存. 

在经典的32位Linux(就是32位虚拟地址空间的Linux), 用户区和内核区在地址0xC0000000处分开, 或者说地址空间的3/4处. 所以用户虚拟地址是从0到0xBFFFFFFF. 剩下的虚拟地址(0xC0000000到 0xFFFFFFFF) 就是内核的虚拟地址空间了.
64位Linux也是类似的拆分, 但有些不同点. 
图 23.2 显示了描述典型（简化）地址空间。

- ![](assets/Pasted%20image%2020230404075830.png)

Linux 的一个稍微有趣的地方是它包含两种类型的内核虚拟地址。
第一个就是内核逻辑地址, 它就是正常的内核虚拟地址空间. 要获取更多这种内存, 内核代码只需要调用kmalloc. 大部分内核数据结构存在这里, 比如页表, 每个进程的内核栈, 等等.
不同于系统中的大多数其他内存, 内核逻辑内存是不能被换到硬盘上的.

内核逻辑地址最有趣的地方是它们与物理内存的连接. 具体来说，内核逻辑地址和物理内存的第一部分之间存在直接映射。
所以, 内核逻辑地址0xC0000000转换成物理地址0x00000000, 0xC0000FFF转换成0x00000FFF, 以此类推..
这种直接映射有两个含义。首先是在内核逻辑地址和物理地址之间来回转换很简单; 于是乎, 这些地址通常被视为确实是物理地址. 
其次是，如果内存块在内核逻辑地址空间中是连续的，那么它在物理内存中也是连续的。
这使得内核地址空间在这一部分分配的内存适用于需要连续物理内存才能正常工作的操作，例如通过直接内存访问 (DMA) 与设备之间的 I/O 传输（我们将在本书的第三部分中了解）。

另一种内核地址, 是内核虚拟地址. 内核代码需要调用一个不同分配函数, vmalloc, 才能获得这种内存. vmalloc返回一个指针, 该指针指向一块申请大小的几乎连续的区域.
不同于内核逻辑内存, 内核虚拟内存通常是不连续的. 每个内核虚拟页都可以映射到不连续的物理页.(因此不适用于DMA). 但是这种内存容易分配, 因此用于大型缓冲区, 其中找到连续的大块物理内存很有挑战性.

在32位Linux中, 内核虚拟地址存在的另一个原因是它们能够让内核寻址超过(大约)1GB的内存. 多年前, 机器的内存要比这少的多, 寻址超过1GB不是问题. 然而，随着技术的进步，很快就需要使内核能够使用更大的内存量. 内核虚拟地址, 不必严格一对一映射到物理内存, 使之成为可能.
但是，随着迁移到 64 位 Linux，需求变得不那么紧迫，因为内核不仅仅限于虚拟地址空间的最后 1 GB。

#### 页表结构

因为聚焦X86 Linux, 我们还是集中于X86提供的页表结构类型. 因为体系结构决定Linux哪些能做哪些不能做. 如前提到的, X86提供一种硬件管理的多级页表结构. 每一个进程有一个页表. 操作系统只需在其内存中设置映射，在页目录的开头指明特权寄存器，硬件处理剩下的操作。
OS参与进程创建, 删除, 上下文切换时确保在任何情况下, 硬件MMU都能用正确的页表来执行转换.

大概这些年最大的变化就是从32位X86转到了64位X86. VAX/VMS也看到了, 32位地址空间持续了很长时间, 随着技术进步, 32位地址空间最终开始成为程序的真正制约. 
虚拟内存使系统编程变得容易，但是对于包含许多GB内存的现代系统，32位不再足以访问全部内存。因此，下一次飞跃变得必要.

来到64位地址, 会按照预期地影响X86中的页表结构. 因为X86用多级页表, 现行64位系统用四级页表. 满64位的虚拟地址空间还没有实际应用, 其实只用了低48位. 看图:

- ![](assets/Pasted%20image%2020230404085514.png)

虚拟地址高16位没有用(因此在地址转换里不扮演任何角色), 最低12位(页大小是4KB)是页内偏移量(不转换, 直接用). 剩下中间36位虚拟地址, 用来参与地址转换.
地址的 P1 部分用于索引到最顶层的页面目录, 转换从那里进行，一次一个级别，直到页表的实际页被 P4 索引，产生所需的页表项.

随着系统内存变得越来越大，这个庞大的地址空间的更多部分将被启用，导致五级，最终六级页表树结构。想象一下：一个简单的页表查找需要六个级别的转换，只是为了找出某个数据段在内存中的位置。

#### 大号页表支持

英特尔X86可以用多种页大小, 不只有标准的4KB页. 最近的设计在硬件上支持2MB 甚至1GB的页. 因此, 随着时间的推移, Linux已经发展到允许应用程序使用这些巨大的页了.

使用大号页, 之前也提到过, 会有很多好处. 首先就是会减少页表里需要的映射数量(页表项), 页越大, 页表项也就越少.
然而, 更少的页表项并不是大号页背后的驱动力, 相反, 它是更好的TLB行为和相关性能提升.

当一个运行的进程使用大量内存, 它就会迅速用地址转换把TLB填满. 如果这些地址转换是针对4KB页的话, 那么要不引起TLB未命中, 就只能访问总内存的一小部分. 对于有些许多GB内存的机器上运行的现代"大内存"工作负载来说, 这个结果显然是性能成本.
最近的研究表明，一些应用程序花费 10% 的周期来维护 TLB 未命中。

大页允许进程访问大块内存而不会TLB未命中，方法是在 TLB 中使用较少的插槽，因此是主要优势。但是，大页面还有其他好处：TLB 未命中路径较短，这意味着当 TLB 未命中发生时，可以更快地对其进行服务。而且, 分配会变得十分快速(某些场景下), 这是个小但有时重要的好处.

Linux 对大页面支持如何逐步完成的? 
起初，Linux 开发人员知道这种支持只对少数应用程序很重要，例如具有严格性能要求的大型数据库。因此, 决定允许应用程序显式请求具有大页的内存分配(通过mmap() 或 shmget()调用). 
这样, 大多数应用不会受到影响(继续使用4KB大小的页), 少部分有需求的应用就必须做出改变去使用这些接口, 但对它们来说还是值得的.

最近，由于许多应用程序中对更好的TLB表现的需求越发普遍，Linux开发人员添加了透明的大页面支持。启用此功能后，操作系统会自动寻找机会分配大页面（通常为 2 MB，但在某些系统上为 1 GB），而无需修改应用程序。

大页并非没有成本. 最大的潜在成本就是内部碎片, 页很大但很少用完, 有一部分被浪费掉..
这种形式的浪费可能以用大而很少使用的页填满内存。如果启用交换，则也不能很好地处理大页面，有时会大大增加系统的 I/O 量。分配开销也可能不好（在其他一些情况下）。
总的来说, 有一点是明确的：多年来为系统提供良好服务的 4 KB 页面大小已不再是曾经的通用解决方案; 不断增长的内存大小要求我们将大页面和其他解决方案视为 VM 系统必要演进的一部分。 Linux对这种基于硬件的技术的缓慢采用证明了即将到来的变化。

#### 页缓存

为了降低访问持久存储的成本（本书第三部分的重点），大多数系统使用积极的缓存子系统将常用数据项保留在内存中。在这方面，Linux与传统操作系统也一样。

linux页缓存是统一的, 把三种主要来源的页保存在内存中. 来源是: 内存映射的文件, 文件数据和元数据(通常把read()和write()调用重定向到文件系统来访问), 以及组成每个进程的堆和栈页(有时称为匿名内存, 因为它下面没有命名文件, 而是交换空间). 
这些实体保存在页面缓存哈希表中，允许在需要所述数据时快速查找。

页缓存追踪 这些项是干净的(读取但未更新), 还是脏的(也就是修改过的). 脏数据由后台线程(称为pdflush)定期写入后备存储(也就是, 文件数据的特定文件, 或者是匿名区域的交换空间), 从而确保修改过的数据最终写回持久存储. 这个后台活动要么在特定时间段后发生, 要么在太多页面被视为脏页时发送(两个都是可配置的参数).

在某些情况下, 系统内存不足, Linux必须决定从内存中踢出哪些页面以释放空间. 为此，Linux 使用修改后的 2Q 替换形式.

基本思想很简单: 标准LRU替换是有效的，但可以通过某些常见的访问模式来颠覆。
例如, 如果一个进程重复地访问一个大文件(尤其是接近内存大小, 或更大的文件), LRU会将所有其他文件都提出内存. 还有更糟的, 把文件的某些部分保留在内存中是没用的, 因为它们在被踢出内存之前永远不会被重新引用.

Linux 版本的 2Q(2 Queues) 替换算法通过保留两个列表并在它们之间分配内存来解决此问题。
首次被访问时, 某页被放在一个队列中(在原本的论文中称为A1, 但在Linux中称为非活动列表);
当它被再次引用时, 该页被提升到另一个队列(原来叫Aq, 但Linux叫活动列表).  
当需要进行替换时, 替换的待选项是从非活动列表获取的.
Linux 还会定期将页面从活动列表的底部移动到非活动列表，使活动列表保持在总页面缓存大小的三分之二左右。

理想情况下, Linux会以完美的LRU顺序管理这些列表, 但是, 这么做成本很高. 因此, 与许多OS一样, 使用LRU的类似算法(类似时钟替换).

这种 2Q 方法的行为通常与 LRU 非常相似，但值得注意的是，通过将循环访问的页面限制为非活动列表来处理循环大文件访问的情况。
由于所述页在被踢出内存之前永远不会被重新引用，因此它们不会清除活动列表中找到的其他有用页面。

>补充: 内存映射的普遍性
>内存映射的出现比Linux早了几年, 并且在Linux和其他现代系统中的许多地方都使用. 
>这个想法很简单：通过在已经打开的文件描述符上调用mmap(), 进程将返回一个指向虚拟内存区域开头的指针, 文件内容似乎位于该区域。
>然后使用该指针, 进程可以通过简单的指针解引用访问文件的任何部分.
>访问尚未放入内存的内存映射文件的部分会触发页错误, 此时, OS将在相关数据中分页, 并且通过相应地更新进程的页表(按需分页)来访问这些数据.
>每个常规的Linux进程都使用内存映射文件, 甚至main()中的代码也不直接调用mmap(), 由于Linux将代码从可执行文件和共享库代码加载到内存中的方式。
>下面是 pmap 命令行工具的（高度缩写）输出，它显示了哪些不同的映射构成了正在运行的程序（在本例中为 tcsh）的虚拟地址空间。
>输出显示四列：映射的虚拟地址、其大小、区域的保护位和映射的源：
>
>- ![](assets/Pasted%20image%2020230404152712.png)
>
>从此输出中可以看到, 来自tcsh二进制文件的代码, 以及来自libc, libcrypt, libtinfo的代码和来自动态链接器本身(ld.so)的代码都映射到地址空间中. 还存在两个匿名区域, 堆(第二个条目, 标记为匿名)和堆栈(标记为堆栈). 内存映射文件为OS构建现代地址空间提供了一种直接有效的方法.

#### 安全和缓冲区溢出

现代虚内存系统(Linux，Solaris或BSD变体之一)和古代VM系统(VAX/VMS)之间的最大区别可能是现代对安全性的强调.
保护一直是操作系统的一个严重问题, 但随着机器比以往任何时候都更加互联, 开发人员实施各种防御对策来阻止那些狡猾的黑客获得对系统的控制也就不足为奇了.

一个主要的威胁是缓冲区溢出攻击, 它可以用于对付普通用户程序甚至内核本身.
这些攻击的想法是在目标系统中找到一个错误，允许攻击者将任意数据注入目标的地址空间。
有时出现此类漏洞是因为开发人员（错误地）假设输入不会太长，因此（可信地）将输入复制到缓冲区中;由于输入实际上太长，它会溢出缓冲区，从而覆盖目标的内存。
像下面这样无辜的代码可能是问题的根源：
```c
int some_function(char *input) {
    char dest_buffer[100];
    strcpy(dest_buffer, input); // oops, unbounded copy!
}
```

在许多情况下，这种溢出不是灾难性的，例如，无辜地提供给用户程序甚至操作系统的错误输入可能会导致它崩溃，但也不会更糟。
但是, 恶意程序员可以小心制作溢出缓冲区的输入, 以便将自己的代码注入目标系统, 本质上允许他们接管它并执行自己的命令。
如果攻击网络连接的用户程序成功, 攻击者可以在受感染的系统上运行任意计算甚至出租周期;如果攻击操作系统本身成功, 攻击可以访问更多资源, 并且是所谓的权限提升的一种形式(即用户代码获得内核访问权限).

防止缓冲区溢出的第一个也是最简单的防御是防止执行在地址空间的某些区域(例如, 在堆栈内)找到的任何代码.

剩下的内容还是安全问题. 看OSTEP1.01英文版.